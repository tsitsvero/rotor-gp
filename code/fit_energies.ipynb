{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit energies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to do:\n",
    "\n",
    "0. Rename FandeDataModuleASE into FandeDataModule\n",
    "1. Improve FandeDataModuleASE, initialization with forces and energies\n",
    "2. make parallel calculation of invariants within"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.expandvars(\"/home/$USER/repos/fande/\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ase import io\n",
    "\n",
    "traj_295 = io.read(\"/data1/simulations/datasets/rotors/high_temp_ML_training_data/results_triasine_ML_2000/struct_295_295K/md_trajectory.traj\", index=\":\")\n",
    "traj_355 = io.read(\"/data1/simulations/datasets/rotors/high_temp_ML_training_data/results_triasine_ML_2000/struct_355_355K/md_trajectory.traj\", index=\":\")\n",
    "\n",
    "traj_295_2000K = io.read(\"/data1/simulations/datasets/rotors/high_temp_ML_training_data/results_triasine_ML_2000/struct_295_2000K/md_trajectory.traj\", index=\":\")\n",
    "traj_355_2000K = io.read(\"/data1/simulations/datasets/rotors/high_temp_ML_training_data/results_triasine_ML_2000/struct_355_2000K/md_trajectory.traj\", index=\":\")\n",
    "traj_295_2000K_forced = io.read(\"/data1/simulations/datasets/rotors/high_temp_ML_training_data/results_triasine_ML_2000/struct_295_2000K_0075force/md_trajectory.traj\", index=\":\")\n",
    "traj_355_2000K_forced = io.read(\"/data1/simulations/datasets/rotors/high_temp_ML_training_data/results_triasine_ML_2000/struct_355_2000K_0075force/md_trajectory.traj\", index=\":\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traj_train = traj_295[0:5000:10] + traj_355[0:5000:10] + traj_295_2000K[0:5000:10] + traj_355_2000K[0:5000:10] + traj_295_2000K_forced[0:5000:10] + traj_355_2000K_forced[0:5000:10]\n",
    "traj_train = traj_295_2000K[1000:5000:5] + traj_355_2000K[1000:5000:5]  \n",
    "traj_train = traj_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energies_train = [s.get_potential_energy() for s in traj_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just to fake the initialization of the fdm\n",
    "# Hyperparameters:\n",
    "hparams = {\n",
    "        'dtype' : 'float32',\n",
    "        'device' : 'gpu'\n",
    "        }\n",
    "\n",
    "from fande.data import FandeDataModuleASE\n",
    "\n",
    "## Train data:\n",
    "energies_train = np.zeros(len(traj_train) )\n",
    "forces_train = np.zeros( (len(traj_train), len(traj_train[0]), 3 ) )\n",
    "\n",
    "for i, snap in enumerate(traj_train):\n",
    "        energies_train[i] = snap.get_potential_energy()\n",
    "        forces_train[i] = snap.get_forces()\n",
    "train_data = {'trajectory': traj_train, 'energies': energies_train,'trajectory_energies': traj_train, 'forces': forces_train}\n",
    "\n",
    "## Test data:\n",
    "# energies_test = np.zeros(len(traj_test) )\n",
    "# forces_test = np.zeros( (len(traj_test), len(traj_test[0]), 3 ) )\n",
    "# for i, snap in enumerate(traj_test):\n",
    "#         energies_test[i] = snap.get_potential_energy()\n",
    "#         forces_test[i] = snap.get_forces()\n",
    "# test_data = {'trajectory_energies': traj_test, 'energies': energies_test, 'forces': forces_test}\n",
    "\n",
    "test_data={}\n",
    "\n",
    "\n",
    "fdm = FandeDataModuleASE(train_data, train_data, hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "metadata": {},
   "outputs": [],
   "source": [
    "# snap_copy = traj_train[0].copy()\n",
    "snap_copy = io.read(\"/data1/simulations/datasets/rotors/simulations_cluster/27_11_2023/27/2023-11-27_17:27:26.502178/md_trajectory.traj\", index=\"0\")\n",
    "\n",
    "from ase.build import make_supercell\n",
    "\n",
    "snap_super = make_supercell(snap_copy, [[1,0,0],[0,1,0],[0,0,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Popen: returncode: None args: ['/home/dlbox2/anaconda3/envs/fande/bin/pytho...>"
      ]
     },
     "execution_count": 690,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ase.visualize import view\n",
    "\n",
    "view(snap_super)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "684"
      ]
     },
     "execution_count": 695,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(snap_super)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of frames is 1\n",
      "Total number of batches is 1\n",
      "Total length of traj is 1\n",
      "Total number of batches 1\n",
      "Calculating invariants on trajectory with librascal...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.96 s, sys: 200 ms, total: 2.16 s\n",
      "Wall time: 1.16 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "\n",
    "# def calculate_invariants_librascal_no_derivatives(\n",
    "#         self,\n",
    "#         trajectory, \n",
    "#         soap_params,\n",
    "#         frames_per_batch=10): \n",
    "\n",
    "# Descriptors parameters:\n",
    "# https://github.com/lab-cosmo/librascal/blob/master/examples/MLIP_example.ipynb\n",
    "# soap_params = {\n",
    "# # 'species': [\"H\", \"C\", \"O\", \"N\", \"Si\"],\n",
    "# 'periodic': True,\n",
    "# 'interaction_cutoff': 3.0,\n",
    "# 'gaussian_sigma_constant': 0.3,\n",
    "# 'max_radial': 5,\n",
    "# 'max_angular': 5,\n",
    "# 'cutoff_smooth_width': 0.1,\n",
    "# # 'average': \"off\",\n",
    "# # 'crossover': True,\n",
    "# # 'dtype': \"float64\",\n",
    "# # 'n_jobs': 10,\n",
    "# # 'sparse': False,\n",
    "# # 'positions': [7, 11, 15] # ignored\n",
    "# }\n",
    "\n",
    "\n",
    "# train_X_np = fdm.calculate_invariants_librascal_no_derivatives(traj_train[0:10], soap_params, frames_per_batch=1)\n",
    "# X = fdm.calculate_invariants_librascal_no_derivatives([snap_super], soap_params, frames_per_batch=1)\n",
    "hypers = dict(soap_type=\"PowerSpectrum\",\n",
    "        interaction_cutoff=3.0,\n",
    "        max_radial=5,\n",
    "        max_angular=5,\n",
    "        gaussian_sigma_constant=0.3,\n",
    "        gaussian_sigma_type=\"Constant\",\n",
    "        cutoff_function_type=\"RadialScaling\",\n",
    "        cutoff_smooth_width=0.1, # 0.1 is way better than 0.5\n",
    "        cutoff_function_parameters=\n",
    "                dict(\n",
    "                        rate=1,\n",
    "                        scale=3.5,\n",
    "                        exponent=4\n",
    "                        ),\n",
    "        radial_basis=\"GTO\",\n",
    "        normalize=True, # setting False makes model untrainable\n",
    "        #   optimization=\n",
    "        #         dict(\n",
    "        #                 Spline=dict(\n",
    "        #                    accuracy=1.0e-05\n",
    "        #                 )\n",
    "        #             ),\n",
    "        compute_gradients=True,\n",
    "        expansion_by_species_method='structure wise'\n",
    "        )\n",
    "        \n",
    "# snap_super.set_pbc([False, False, False])\n",
    "snap_super.set_pbc([True, True, True])\n",
    "# snap_super\n",
    "\n",
    "fdm.soap_hypers = hypers\n",
    "fdm.atomic_groups_train = [list(range(len(snap_super)))]\n",
    "fdm.centers_positions_train = [list(range(len(snap_super)))]\n",
    "fdm.derivatives_positions_train = [list(range(len(snap_super)))]\n",
    "X = fdm.calculate_invariants_librascal_no_derivatives([snap_super], soap_params, frames_per_batch=1)\n",
    "# DX = fdm.calculate_snapshot_invariants_librascal(snap_super, same_centers_derivatives=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_non_H = []\n",
    "\n",
    "for ind,s in enumerate(traj_train[0].get_chemical_symbols()):\n",
    "        # print(s)\n",
    "        if s != \"H\":\n",
    "                indices_non_H.append(ind)\n",
    "\n",
    "energies_train_normalized = (energies_train - np.min(energies_train))/(np.max(energies_train) - np.min(energies_train))\n",
    "\n",
    "# train_X = X_np[:,indices_non_H,:]\n",
    "train_X = train_X_np\n",
    "\n",
    "# train_X = torch.tensor(X_np_batched)\n",
    "train_X = torch.tensor(train_X.sum(axis=1)).cuda()\n",
    "train_Y = torch.tensor(energies_train_normalized).cuda()\n",
    "\n",
    "print(train_X.shape, train_Y.shape, test_X.shape, test_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gpytorch\n",
    "import numpy as np\n",
    "\n",
    "from gpytorch.kernels import (\n",
    "    RBFKernel,\n",
    "    ScaleKernel,\n",
    "    LinearKernel,\n",
    "    AdditiveKernel,\n",
    "    MultitaskKernel,\n",
    "    PolynomialKernel,\n",
    "    MaternKernel,\n",
    ")\n",
    "from gpytorch.means import ZeroMean, ConstantMean\n",
    "\n",
    "from gpytorch.models import ExactGP\n",
    "\n",
    "import torch, torch.nn as nn\n",
    "\n",
    "\n",
    "# https://stackoverflow.com/questions/76251549/setting-the-task-covariance-matrix-to-the-correlation-matrix-in-gpytorch\n",
    "\n",
    "class ExactGPModelEnergies(ExactGP):\n",
    "    def __init__(self, train_X, train_Y, likelihood):\n",
    "        super().__init__(\n",
    "            train_X, train_Y, likelihood\n",
    "        )  # the old-style super(ExactGPModel, self) was causing error!\n",
    "        self.mean_module = ConstantMean()\n",
    "\n",
    "        self.soap_dim = train_X.shape[-1]\n",
    "        # self.covar_module = ScaleKernel(RBFKernel(ard_num_dims=self.soap_dim))#LinearKernel()\n",
    "        self.covar_module = ScaleKernel(MaternKernel(ard_num_dims=self.soap_dim))#LinearKernel()\n",
    "        # self.covar_module = ScaleKernel(LinearKernel(ard_num_dims=self.soap_dim))\n",
    "\n",
    "    def forward(self, X):\n",
    "        x = X\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_X = train_X.cuda()\n",
    "# train_Y = torch.tensor(energies_train_normalized).cuda()\n",
    "# test_X = test_X.cuda()\n",
    "# test_Y = test_Y.cuda()\n",
    "# initialize likelihood and model\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = ExactGPModelEnergies(train_X, train_Y, likelihood)\n",
    "# model = ExactGPModelEnergiesMulti(train_X, train_Y, likelihood)\n",
    "model = model.cuda()\n",
    "likelihood = likelihood.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is for running the notebook in our testing framework\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "training_iter = 100\n",
    "\n",
    "\n",
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)  # Includes GaussianLikelihood parameters\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "print(\"Training...\")\n",
    "# initial_loss = -mll(model(train_X), train_Y).item()\n",
    "for i in range(training_iter):\n",
    "    # Zero gradients from previous iteration\n",
    "    optimizer.zero_grad()\n",
    "    # print(\"grad zeroed\")\n",
    "    # Output from model\n",
    "    output = model(train_X)\n",
    "    # print(\"Output calculated\")\n",
    "    # print(output)\n",
    "    # Calc loss and backprop gradients\n",
    "    loss = -mll(output, train_Y)\n",
    "    # print(\"Loss calculated, \")\n",
    "\n",
    "    loss.backward()\n",
    "    # print(loss)\n",
    "    # print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f   noise: %.3f' % (\n",
    "    #     i + 1, training_iter, loss.item(),\n",
    "    #     model.covar_module.base_kernel.lengthscale.item(),\n",
    "    #     model.likelihood.noise.item()\n",
    "    # ))\n",
    "    print(f\"step: {i},loss: {loss}\")\n",
    "    optimizer.step()\n",
    "\n",
    "# print(\"Initial loss: \", initial_loss)\n",
    "print(\"Final loss: \", loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# edit     def calculate_invariants_librascal_no_derivatives("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get into evaluation (predictive posterior) mode\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "test_x = test_X\n",
    "\n",
    "# Test points are regularly spaced along [0,1]\n",
    "# Make predictions by feeding model through likelihood\n",
    "# with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "#     test_x = torch.linspace(0, 1, 51)\n",
    "observed_pred = likelihood(model(test_x)).mean.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(observed_pred, label=\"Predicted energy\")\n",
    "plt.plot(test_Y.detach().cpu().numpy(), label=\"True energy\")\n",
    "plt.xlabel('step')\n",
    "plt.ylabel('Predicted Energy')\n",
    "# plt.xlim(0, 100)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_Y)\n",
    "plt.ylabel('Energy')\n",
    "plt.xlabel('step')\n",
    "plt.xlim(85, 100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_test = io.read(\"/data1/simulations/datasets/rotors/for jacs paper/295_0.075_same_+/md_trajectory.traj\", index=\":\", format=\"traj\")\n",
    "energies_test = [s.get_potential_energy() for s in traj_test]\n",
    "energies_test_normalized = (energies_test - np.min(energies_train))/(np.max(energies_train) - np.min(energies_train))\n",
    "traj = traj_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X_np = fdm.calculate_invariants_librascal_no_derivatives(traj_test[0:10], soap_params, frames_per_batch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_non_H = []\n",
    "\n",
    "for ind,s in enumerate(traj[0].get_chemical_symbols()):\n",
    "        # print(s)\n",
    "        if s != \"H\":\n",
    "                indices_non_H.append(ind)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "X_np_batched = np.array(X_np_batched)\n",
    "E_np_batched = np.array(E_np_batched).flatten()\n",
    "\n",
    "# test_X = X_np_batched[:,indices_non_H,:]\n",
    "test_X = X_np_batched\n",
    "test_X = test_X.sum(axis=1)\n",
    "test_X = torch.tensor(test_X).cuda()\n",
    "test_Y = torch.tensor(energies_test_normalized).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.plot(energies_train_normalized)\n",
    "# plt.plot(energies_test_normalized)\n",
    "plt.plot(test_Y.detach().cpu().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drafting the energy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.expandvars(\"/home/$USER/repos/fande/\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ase import io\n",
    "\n",
    "traj_295 = io.read(\"/data1/simulations/datasets/rotors/high_temp_ML_training_data/results_triasine_ML_2000/struct_295_295K/md_trajectory.traj\", index=\":\")\n",
    "traj_355 = io.read(\"/data1/simulations/datasets/rotors/high_temp_ML_training_data/results_triasine_ML_2000/struct_355_355K/md_trajectory.traj\", index=\":\")\n",
    "\n",
    "traj_295_2000K = io.read(\"/data1/simulations/datasets/rotors/high_temp_ML_training_data/results_triasine_ML_2000/struct_295_2000K/md_trajectory.traj\", index=\":\")\n",
    "traj_355_2000K = io.read(\"/data1/simulations/datasets/rotors/high_temp_ML_training_data/results_triasine_ML_2000/struct_355_2000K/md_trajectory.traj\", index=\":\")\n",
    "traj_295_2000K_forced = io.read(\"/data1/simulations/datasets/rotors/high_temp_ML_training_data/results_triasine_ML_2000/struct_295_2000K_0075force/md_trajectory.traj\", index=\":\")\n",
    "traj_355_2000K_forced = io.read(\"/data1/simulations/datasets/rotors/high_temp_ML_training_data/results_triasine_ML_2000/struct_355_2000K_0075force/md_trajectory.traj\", index=\":\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fande.models module imported...\n"
     ]
    }
   ],
   "source": [
    "from fande.data import FandeDataModule\n",
    "\n",
    "fdm = FandeDataModule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length of traj is 300\n",
      "Total number of batches 300\n",
      "Calculating invariants on trajectory with librascal...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:11<00:00, 26.17it/s]\n",
      "/home/dlbox2/repos/fande/fande/data/data_module.py:186: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343995026/work/torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  X = torch.tensor(X_np_batched,dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "soap_params = dict(soap_type=\"PowerSpectrum\",\n",
    "        interaction_cutoff=1.0,\n",
    "        max_radial=1,\n",
    "        max_angular=1,\n",
    "        gaussian_sigma_constant=0.3,\n",
    "        gaussian_sigma_type=\"Constant\",\n",
    "        cutoff_function_type=\"RadialScaling\",\n",
    "        cutoff_smooth_width=0.1, # 0.1 is way better than 0.5\n",
    "        cutoff_function_parameters=\n",
    "                dict(\n",
    "                        rate=1,\n",
    "                        scale=3.5,\n",
    "                        exponent=4\n",
    "                        ),\n",
    "        radial_basis=\"GTO\",\n",
    "        normalize=True, # setting False makes model untrainable\n",
    "        #   optimization=\n",
    "        #         dict(\n",
    "        #                 Spline=dict(\n",
    "        #                    accuracy=1.0e-05\n",
    "        #                 )\n",
    "        #             ),\n",
    "        compute_gradients=True,\n",
    "        expansion_by_species_method='structure wise'\n",
    "        )\n",
    "\n",
    "rate = 100\n",
    "# traj_train = traj_295[0:5000:10] + traj_355[0:5000:10] + traj_295_2000K[0:5000:10] + traj_355_2000K[0:5000:10] + traj_295_2000K_forced[0:5000:10] + traj_355_2000K_forced[0:5000:10]\n",
    "traj_train = traj_295[0:5000:rate] + traj_355[0:5000:rate] + traj_295_2000K[0:5000:rate] + traj_355_2000K[0:5000:rate] + traj_295_2000K_forced[0:5000:rate] + traj_355_2000K_forced[0:5000:rate]\n",
    "\n",
    "train_energies = [s.calc.get_potential_energy() for s in traj_train]\n",
    "emax = max(train_energies)\n",
    "emin = min(train_energies)\n",
    "train_energies = [(e - emin)/(emax - emin) for e in train_energies]\n",
    "\n",
    "fdm.soap_hypers = soap_params\n",
    "fdm.atomic_groups_train = [list(range(len(traj_train[0])))]\n",
    "train_data_loader = fdm.prepare_train_data_loaders_energy(traj_train, train_energies, soap_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dlbox2/anaconda3/envs/fande/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:165: PossibleUserWarning: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/dlbox2/anaconda3/envs/fande/lib/python3.10/sit ...\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RawEnergyModel initialized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dlbox2/anaconda3/envs/fande/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "  warning_cache.warn(\n",
      "/home/dlbox2/anaconda3/envs/fande/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:196: UserWarning: Attribute 'energy_model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['energy_model'])`.\n",
      "  rank_zero_warn(\n"
     ]
    }
   ],
   "source": [
    "from fande.models import EnergyModel, RawEnergyModel\n",
    "\n",
    "#     def __init__(\n",
    "#             self,\n",
    "#             energy_model,\n",
    "#             energy_train_data_loaders: list,\n",
    "#             fdm=None, # specification of fdm is optional\n",
    "#             hparams=None,\n",
    "#             gpu_id=None\n",
    "\n",
    "hparams = {\n",
    "        'dtype' : 'float32',\n",
    "        'device' : 'gpu',\n",
    "        'energy_model_hparams' : {\n",
    "                'num_epochs' : 100,\n",
    "                'learning_rate' : 0.01,\n",
    "        }\n",
    "        }\n",
    "\n",
    "\n",
    "raw_energy_model = RawEnergyModel(\n",
    "        train_x = train_data_loader.dataset[:][0],\n",
    "        train_y = train_data_loader.dataset[:][1],\n",
    "        hparams = hparams)\n",
    "\n",
    "        \n",
    "Energy_model = EnergyModel(\n",
    "        raw_energy_model,\n",
    "        train_data_loader,\n",
    "        hparams=hparams, \n",
    "        gpu_id=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training energy model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dlbox2/anaconda3/envs/fande/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:165: PossibleUserWarning: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/dlbox2/anaconda3/envs/fande/lib/python3.10/sit ...\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name       | Type                       | Params\n",
      "----------------------------------------------------------\n",
      "0 | likelihood | GaussianLikelihood         | 1     \n",
      "1 | model      | ExactGPModelEnergy         | 32    \n",
      "2 | mll        | ExactMarginalLogLikelihood | 32    \n",
      "----------------------------------------------------------\n",
      "32        Trainable params\n",
      "0         Non-trainable params\n",
      "32        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "/home/dlbox2/anaconda3/envs/fande/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 128 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/dlbox2/anaconda3/envs/fande/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:280: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12fe4d3248a849149f62f5fac57bca34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    }
   ],
   "source": [
    "Energy_model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fande.predict import FandePredictor\n",
    "from fande.ase import FandeCalc\n",
    "\n",
    "AG_force_model = None\n",
    "predictor = FandePredictor(\n",
    "        fdm,\n",
    "        AG_force_model,\n",
    "        Energy_model,\n",
    "        hparams,\n",
    "        soap_params\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "[0.14562514] [0.30684346]\n"
     ]
    }
   ],
   "source": [
    "energy, energy_uncertainty = predictor.predict_energy_single_snapshot_r(traj_train[0].copy())\n",
    "print(energy, energy_uncertainty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "fande_calc = FandeCalc(predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atomic group force model is not defined. Cannot predict forces. Returning zeros.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.14562514], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atoms = traj_train[33].copy()\n",
    "\n",
    "atoms.set_calculator(fande_calc)\n",
    "\n",
    "atoms.get_potential_energy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving predictor requires humongous amount of memory! Spare some dozens of GBs!\n"
     ]
    }
   ],
   "source": [
    "fande_calc.save_predictor(\"prrrrr.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fande.models module imported...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from fande.ase import FandeCalc\n",
    "\n",
    "predictor_loaded = torch.load(\"prrrrr.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fande_calc_loaded = FandeCalc(predictor_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ase import io\n",
    "atoms = io.read(\"/data1/simulations/datasets/rotors/high_temp_ML_training_data/results_triasine_ML_2000/struct_295_295K/md_trajectory.traj\", index=\"10\")\n",
    "\n",
    "\n",
    "atoms.set_calculator(fande_calc_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atomic group force model is not defined. Cannot predict forces. Returning zeros.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dlbox2/repos/fande/fande/data/data_module.py:435: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343995026/work/torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  X = torch.tensor(X_np_batched,dtype=torch.float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.14562514], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atoms.get_potential_energy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atoms.get_forces()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
