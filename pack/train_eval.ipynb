{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dCOI-tn1woN"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tsitsvero/rotor-gp/blob/main/pack/train_eval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGkUAB7q1woR"
      },
      "source": [
        "# Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znyGV5dF1woR",
        "outputId": "02c09e0a-fadb-4309-dac3-e805c5322f4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/lab-cosmo/librascal\n",
            "  Cloning https://github.com/lab-cosmo/librascal to /tmp/pip-req-build-ubbxnr28\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/lab-cosmo/librascal /tmp/pip-req-build-ubbxnr28\n",
            "  Resolved https://github.com/lab-cosmo/librascal to commit 6c55e99720f9a181f1efc5c7cd4976c87809d79b\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting git+https://github.com/chem-gp/fande\n",
            "  Cloning https://github.com/chem-gp/fande to /tmp/pip-req-build-vsccjk_a\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/chem-gp/fande /tmp/pip-req-build-vsccjk_a\n",
            "  Resolved https://github.com/chem-gp/fande to commit aded96b5f6a1eec9b699457a47248bcb58f290d1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: gpytorch in /usr/local/lib/python3.10/dist-packages (1.11)\n",
            "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.10/dist-packages (2.1.3)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.16.1)\n",
            "Requirement already satisfied: xtb in /usr/local/lib/python3.10/dist-packages (22.1)\n",
            "Requirement already satisfied: nbstripout in /usr/local/lib/python3.10/dist-packages (0.6.1)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from rascal==0.0.0) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from rascal==0.0.0) (1.11.4)\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from rascal==0.0.0) (3.7.1)\n",
            "Requirement already satisfied: ase>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from rascal==0.0.0) (3.22.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from gpytorch) (1.2.2)\n",
            "Requirement already satisfied: linear-operator>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from gpytorch) (0.5.2)\n",
            "Requirement already satisfied: torch>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2.1.0+cu121)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.66.1)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (6.0.1)\n",
            "Requirement already satisfied: fsspec[http]>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2023.6.0)\n",
            "Requirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (1.2.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (23.2)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.5.0)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (0.10.0)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.40)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.39.1)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from xtb) (1.16.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from nbstripout) (5.9.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.9.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: jaxtyping>=0.2.9 in /usr/local/lib/python3.10/dist-packages (from linear-operator>=0.5.0->gpytorch) (0.2.25)\n",
            "Requirement already satisfied: typeguard~=2.13.3 in /usr/local/lib/python3.10/dist-packages (from linear-operator>=0.5.0->gpytorch) (2.13.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->rascal==0.0.0) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->rascal==0.0.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->rascal==0.0.0) (4.46.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->rascal==0.0.0) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->rascal==0.0.0) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->rascal==0.0.0) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->rascal==0.0.0) (2.8.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2023.11.17)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch-lightning) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch-lightning) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch-lightning) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch-lightning) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch-lightning) (2.1.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->xtb) (2.21)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat->nbstripout) (2.19.0)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat->nbstripout) (4.19.2)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.10/dist-packages (from nbformat->nbstripout) (5.5.1)\n",
            "Requirement already satisfied: traitlets>=5.1 in /usr/local/lib/python3.10/dist-packages (from nbformat->nbstripout) (5.7.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->gpytorch) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->gpytorch) (3.2.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (4.0.3)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->nbstripout) (2023.11.2)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->nbstripout) (0.32.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->nbstripout) (0.15.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.12.0->pytorch-lightning) (2.1.3)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core->nbformat->nbstripout) (4.1.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.12.0->pytorch-lightning) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/lab-cosmo/librascal gpytorch pytorch-lightning wandb xtb nbstripout\n",
        "!pip install git+https://github.com/chem-gp/fande --force-reinstall\n",
        "# ! nbstripout --install --global"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Davwyq8M136x",
        "outputId": "17e8530b-7264-4728-eb57-f9c6c8e5877a"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1vtf8fz1woT"
      },
      "source": [
        "## Cooking up the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRz1N5Qq1woU",
        "outputId": "0d085c11-578b-4e93-9d3b-91bf1563ecdc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DATA_DIR /content/drive/MyDrive/data/\n",
            "RESULTS_DIR /content/results\n",
            "ENERGY_MODEL exact\n",
            "ENERGY_NUM_INDUCING_POINTS 500\n",
            "ENERGY_LR 0.01\n",
            "ENERGY_NUM_STEPS 2000\n",
            "FORCES_MODEL exact\n",
            "FORCES_NUM_INDUCING_POINTS 500\n",
            "NUM_FORCE_SAMPLES 1000\n",
            "FORCES_LR 0.01\n",
            "FORCES_NUM_STEPS 2000\n",
            "PREDICTOR_NAME test.pth\n",
            "500 500\n",
            "fande.models module imported...\n",
            "Icecream logger is not available\n",
            "Total number of found groups: 14\n",
            "Checking if all atoms are covered:  True\n",
            "Total length of traj is 500\n",
            "Total number of batches 500\n",
            "Calculating invariants on trajectory with librascal...\n",
            "100% 500/500 [00:13<00:00, 36.66it/s]\n",
            "invariants for energy fitting calculated\n",
            "100% 500/500 [09:03<00:00,  1.09s/it]\n",
            "invariants for forces fitting calculated\n",
            "torch.Size([500, 1200])\n",
            "torch.Size([500])\n",
            "Dataloader for group 0 created\n",
            "Number of samples in dataloader: 1000\n",
            "Dataloader for group 1 created\n",
            "Number of samples in dataloader: 1000\n",
            "Dataloader for group 2 created\n",
            "Number of samples in dataloader: 1000\n",
            "Dataloader for group 3 created\n",
            "Number of samples in dataloader: 1000\n",
            "Dataloader for group 4 created\n",
            "Number of samples in dataloader: 1000\n",
            "Dataloader for group 5 created\n",
            "Number of samples in dataloader: 1000\n",
            "Dataloader for group 6 created\n",
            "Number of samples in dataloader: 1000\n",
            "Dataloader for group 7 created\n",
            "Number of samples in dataloader: 1000\n",
            "Dataloader for group 8 created\n",
            "Number of samples in dataloader: 1000\n",
            "Dataloader for group 9 created\n",
            "Number of samples in dataloader: 1000\n",
            "Dataloader for group 10 created\n",
            "Number of samples in dataloader: 1000\n",
            "Dataloader for group 11 created\n",
            "Number of samples in dataloader: 1000\n",
            "Dataloader for group 12 created\n",
            "Number of samples in dataloader: 1000\n",
            "Dataloader for group 13 created\n",
            "Number of samples in dataloader: 1000\n",
            "\n",
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "Training energy model\n",
            "2023-12-30 16:57:49.050831: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-30 16:57:49.051012: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-30 16:57:49.136476: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-30 16:57:52.724564: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\n",
            "  | Name       | Type                       | Params\n",
            "----------------------------------------------------------\n",
            "0 | likelihood | GaussianLikelihood         | 1     \n",
            "1 | model      | ExactGPModel               | 1.2 K \n",
            "2 | mll        | ExactMarginalLogLikelihood | 1.2 K \n",
            "----------------------------------------------------------\n",
            "1.2 K     Trainable params\n",
            "0         Non-trainable params\n",
            "1.2 K     Total params\n",
            "0.005     Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py:293: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=1000). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "Epoch 1999: 100% 1/1 [00:00<00:00, 25.81it/s, v_num=60, loss=-2.61]`Trainer.fit` stopped: `max_epochs=2000` reached.\n",
            "Epoch 1999: 100% 1/1 [00:00<00:00, 21.57it/s, v_num=60, loss=-2.61]\n",
            "Energy model fitted\n",
            "\n",
            "ModelForces initialized\n",
            "\n",
            "ModelForces initialized\n",
            "\n",
            "ModelForces initialized\n",
            "\n",
            "ModelForces initialized\n",
            "\n",
            "ModelForces initialized\n",
            "\n",
            "ModelForces initialized\n",
            "\n",
            "ModelForces initialized\n",
            "\n",
            "ModelForces initialized\n",
            "\n",
            "ModelForces initialized\n",
            "\n",
            "ModelForces initialized\n",
            "\n",
            "ModelForces initialized\n",
            "\n",
            "ModelForces initialized\n",
            "\n",
            "ModelForces initialized\n",
            "\n",
            "ModelForces initialized\n",
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "Training force model 0 (Total 14 models)\n",
            "\n",
            "  | Name       | Type                       | Params\n",
            "----------------------------------------------------------\n",
            "0 | likelihood | GaussianLikelihood         | 1     \n",
            "1 | model      | ExactGPModel               | 1.2 K \n",
            "2 | mll        | ExactMarginalLogLikelihood | 1.2 K \n",
            "----------------------------------------------------------\n",
            "1.2 K     Trainable params\n",
            "0         Non-trainable params\n",
            "1.2 K     Total params\n",
            "0.005     Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py:293: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "Epoch 1999: 100% 1/1 [00:00<00:00,  5.51it/s, v_num=61, loss=0.967]`Trainer.fit` stopped: `max_epochs=2000` reached.\n",
            "Epoch 1999: 100% 1/1 [00:00<00:00,  5.38it/s, v_num=61, loss=0.967]\n",
            "Training force model 1 (Total 14 models)\n",
            "\n",
            "  | Name       | Type                       | Params\n",
            "----------------------------------------------------------\n",
            "0 | likelihood | GaussianLikelihood         | 1     \n",
            "1 | model      | ExactGPModel               | 1.2 K \n",
            "2 | mll        | ExactMarginalLogLikelihood | 1.2 K \n",
            "----------------------------------------------------------\n",
            "1.2 K     Trainable params\n",
            "0         Non-trainable params\n",
            "1.2 K     Total params\n",
            "0.005     Total estimated model params size (MB)\n",
            "Epoch 1999: 100% 1/1 [00:00<00:00,  3.24it/s, v_num=62, loss=0.351]`Trainer.fit` stopped: `max_epochs=2000` reached.\n",
            "Epoch 1999: 100% 1/1 [00:00<00:00,  3.20it/s, v_num=62, loss=0.351]\n",
            "Training force model 2 (Total 14 models)\n",
            "\n",
            "  | Name       | Type                       | Params\n",
            "----------------------------------------------------------\n",
            "0 | likelihood | GaussianLikelihood         | 1     \n",
            "1 | model      | ExactGPModel               | 1.2 K \n",
            "2 | mll        | ExactMarginalLogLikelihood | 1.2 K \n",
            "----------------------------------------------------------\n",
            "1.2 K     Trainable params\n",
            "0         Non-trainable params\n",
            "1.2 K     Total params\n",
            "0.005     Total estimated model params size (MB)\n",
            "Epoch 1999: 100% 1/1 [00:00<00:00,  4.58it/s, v_num=63, loss=1.010]`Trainer.fit` stopped: `max_epochs=2000` reached.\n",
            "Epoch 1999: 100% 1/1 [00:00<00:00,  4.51it/s, v_num=63, loss=1.010]\n",
            "Training force model 3 (Total 14 models)\n",
            "\n",
            "  | Name       | Type                       | Params\n",
            "----------------------------------------------------------\n",
            "0 | likelihood | GaussianLikelihood         | 1     \n",
            "1 | model      | ExactGPModel               | 1.2 K \n",
            "2 | mll        | ExactMarginalLogLikelihood | 1.2 K \n",
            "----------------------------------------------------------\n",
            "1.2 K     Trainable params\n",
            "0         Non-trainable params\n",
            "1.2 K     Total params\n",
            "0.005     Total estimated model params size (MB)\n",
            "Epoch 1999: 100% 1/1 [00:00<00:00,  3.89it/s, v_num=64, loss=0.645]`Trainer.fit` stopped: `max_epochs=2000` reached.\n",
            "Epoch 1999: 100% 1/1 [00:00<00:00,  3.84it/s, v_num=64, loss=0.645]\n",
            "Training force model 4 (Total 14 models)\n",
            "\n",
            "  | Name       | Type                       | Params\n",
            "----------------------------------------------------------\n",
            "0 | likelihood | GaussianLikelihood         | 1     \n",
            "1 | model      | ExactGPModel               | 1.2 K \n",
            "2 | mll        | ExactMarginalLogLikelihood | 1.2 K \n",
            "----------------------------------------------------------\n",
            "1.2 K     Trainable params\n",
            "0         Non-trainable params\n",
            "1.2 K     Total params\n",
            "0.005     Total estimated model params size (MB)\n",
            "Epoch 1999: 100% 1/1 [00:00<00:00,  4.25it/s, v_num=65, loss=0.980]`Trainer.fit` stopped: `max_epochs=2000` reached.\n",
            "Epoch 1999: 100% 1/1 [00:00<00:00,  4.17it/s, v_num=65, loss=0.980]\n",
            "Training force model 5 (Total 14 models)\n",
            "\n",
            "  | Name       | Type                       | Params\n",
            "----------------------------------------------------------\n",
            "0 | likelihood | GaussianLikelihood         | 1     \n",
            "1 | model      | ExactGPModel               | 1.2 K \n",
            "2 | mll        | ExactMarginalLogLikelihood | 1.2 K \n",
            "----------------------------------------------------------\n",
            "1.2 K     Trainable params\n",
            "0         Non-trainable params\n",
            "1.2 K     Total params\n",
            "0.005     Total estimated model params size (MB)\n",
            "Epoch 1565:   0% 0/1 [00:00<?, ?it/s, v_num=66, loss=0.198]/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
            "Training force model 6 (Total 14 models)\n",
            "\n",
            "  | Name       | Type                       | Params\n",
            "----------------------------------------------------------\n",
            "0 | likelihood | GaussianLikelihood         | 1     \n",
            "1 | model      | ExactGPModel               | 1.2 K \n",
            "2 | mll        | ExactMarginalLogLikelihood | 1.2 K \n",
            "----------------------------------------------------------\n",
            "1.2 K     Trainable params\n",
            "0         Non-trainable params\n",
            "1.2 K     Total params\n",
            "0.005     Total estimated model params size (MB)\n",
            "Epoch 58:   0% 0/1 [00:00<?, ?it/s, v_num=67, loss=1.590][rank: 0] Received SIGTERM: 15\n",
            "[rank: 0] Received SIGTERM: 15\n",
            "Training force model 7 (Total 14 models)\n",
            "\n",
            "  | Name       | Type                       | Params\n",
            "----------------------------------------------------------\n",
            "0 | likelihood | GaussianLikelihood         | 1     \n",
            "1 | model      | ExactGPModel               | 1.2 K \n",
            "2 | mll        | ExactMarginalLogLikelihood | 1.2 K \n",
            "----------------------------------------------------------\n",
            "1.2 K     Trainable params\n",
            "0         Non-trainable params\n",
            "1.2 K     Total params\n",
            "0.005     Total estimated model params size (MB)\n",
            "Epoch 2:   0% 0/1 [00:00<?, ?it/s, v_num=68, loss=1.290]^C\n",
            "CPU times: user 3min 6s, sys: 15.2 s, total: 3min 21s\n",
            "Wall time: 52min 22s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "import os\n",
        "# DATA_DIR = os.path.expanduser(\"~/repos/data/\")\n",
        "# # DATA_DIR = \"/data1/simulations/datasets/rotors/high_temp_ML_training_data/\"\n",
        "# RESULTS_DIR = os.path.expanduser(\"~/repos/data/results\")\n",
        "\n",
        "\n",
        "DATA_DIR = os.path.expanduser(\"/content/drive/MyDrive/data/\")\n",
        "# FANDE_DIR = os.path.expanduser(\"~/\")\n",
        "RESULTS_DIR = os.path.expanduser(\"/content/results\")\n",
        "# os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "ENERGY_MODEL = 'exact' #'variational_inducing_points', 'exact'\n",
        "ENERGY_NUM_INDUCING_POINTS = 500\n",
        "ENERGY_LR = 0.01\n",
        "ENERGY_NUM_STEPS = 2000\n",
        "\n",
        "FORCES_MODEL = 'exact' #'variational_inducing_points', 'exact'\n",
        "FORCES_NUM_INDUCING_POINTS = 500\n",
        "NUM_FORCE_SAMPLES = 1000\n",
        "FORCES_LR = 0.01\n",
        "FORCES_NUM_STEPS = 2000\n",
        "\n",
        "PREDICTOR_NAME = 'test.pth'\n",
        "SUBSAMPLE = 2 # subsample data to reduce time durings tests\n",
        "\n",
        "! python cook_model.py \\\n",
        "--data_dir $DATA_DIR \\\n",
        "--results_dir $RESULTS_DIR \\\n",
        "--energy_model $ENERGY_MODEL \\\n",
        "--energy_num_inducing_points $ENERGY_NUM_INDUCING_POINTS \\\n",
        "--energy_lr $ENERGY_LR \\\n",
        "--energy_num_steps $ENERGY_NUM_STEPS \\\n",
        "--forces_model $FORCES_MODEL \\\n",
        "--forces_num_inducing_points $FORCES_NUM_INDUCING_POINTS \\\n",
        "--num_force_samples $NUM_FORCE_SAMPLES \\\n",
        "--forces_lr $FORCES_LR \\\n",
        "--forces_num_steps $FORCES_NUM_STEPS \\\n",
        "--predictor_name $PREDICTOR_NAME \\\n",
        "--subsample $SUBSAMPLE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ry9M9pUc1woU"
      },
      "source": [
        "## Testing models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "Q-igKC1a1woV"
      },
      "outputs": [],
      "source": [
        "from ase import io\n",
        "test_traj = io.read(DATA_DIR + \"/results_triasine_ML_2000/struct_295_295K/md_trajectory.traj\", index=\"1000:1010\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "81lptjK31woV"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import torch\n",
        "FANDE_DIR = os.path.expanduser(\"~/repos/\")\n",
        "sys.path.append(FANDE_DIR + \"fande/\")\n",
        "\n",
        "from fande.predict import FandePredictor\n",
        "from fande.ase import FandeCalc\n",
        "# load the predictor:\n",
        "# predictor_loaded = torch.load(RESULTS_DIR + \"/fande_predictor.pth\")\n",
        "predictor_loaded = torch.load(RESULTS_DIR + \"/test.pth\")\n",
        "fande_calc_loaded = FandeCalc(predictor_loaded)\n",
        "device = torch.device('cpu')\n",
        "fande_calc_loaded.predictor.move_models_to_device(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nc-Rh9361woX",
        "outputId": "777321d6-1d91-4c86-dc43-d4fc8b99399c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fande:Setting context for descriptors calculation to production\n",
            "INFO:fande:Setting context for descriptors calculation to production\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time for invariants (call from forces):  1366.1811351776123\n",
            "Predicting forces...\n",
            "Energy model summary: \n",
            "Time invariants:  0.23984909057617188\n",
            "Time prediction:  100.20017623901367\n",
            "Time moving on device:  0.732421875\n",
            "Time total:  109.12299156188965\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fande:Setting context for descriptors calculation to production\n",
            "INFO:fande:Setting context for descriptors calculation to production\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time for invariants (call from forces):  1699.1944313049316\n",
            "Predicting forces...\n",
            "Energy model summary: \n",
            "Time invariants:  0.1990795135498047\n",
            "Time prediction:  9.5062255859375\n",
            "Time moving on device:  1.4545917510986328\n",
            "Time total:  14.257192611694336\n",
            "Time for invariants (call from forces):  1776.6029834747314\n",
            "Predicting forces...\n",
            "Energy model summary: \n",
            "Time invariants:  0.18525123596191406\n",
            "Time prediction:  24.628639221191406\n",
            "Time moving on device:  0.8378028869628906\n",
            "Time total:  40.886878967285156\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fande:Setting context for descriptors calculation to production\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time for invariants (call from forces):  1750.6988048553467\n",
            "Predicting forces...\n",
            "Energy model summary: \n",
            "Time invariants:  0.1976490020751953\n",
            "Time prediction:  7.732868194580078\n",
            "Time moving on device:  0.8630752563476562\n",
            "Time total:  16.45350456237793\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fande:Setting context for descriptors calculation to production\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time for invariants (call from forces):  1837.3310565948486\n",
            "Predicting forces...\n",
            "Energy model summary: \n",
            "Time invariants:  0.19073486328125\n",
            "Time prediction:  5.807399749755859\n",
            "Time moving on device:  0.6337165832519531\n",
            "Time total:  21.947860717773438\n",
            "Time for invariants (call from forces):  1713.8493061065674\n",
            "Predicting forces...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fande:Setting context for descriptors calculation to production\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Energy model summary: \n",
            "Time invariants:  0.15783309936523438\n",
            "Time prediction:  112.13397979736328\n",
            "Time moving on device:  0.8337497711181641\n",
            "Time total:  124.3743896484375\n",
            "Time for invariants (call from forces):  1676.2523651123047\n",
            "Predicting forces...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fande:Setting context for descriptors calculation to production\n",
            "INFO:fande:Setting context for descriptors calculation to production\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Energy model summary: \n",
            "Time invariants:  0.18930435180664062\n",
            "Time prediction:  74.32031631469727\n",
            "Time moving on device:  6.214380264282227\n",
            "Time total:  105.34095764160156\n",
            "Time for invariants (call from forces):  1175.8182048797607\n",
            "Predicting forces...\n",
            "Energy model summary: \n",
            "Time invariants:  0.2951622009277344\n",
            "Time prediction:  9.64212417602539\n",
            "Time moving on device:  0.8225440979003906\n",
            "Time total:  18.388032913208008\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fande:Setting context for descriptors calculation to production\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time for invariants (call from forces):  1725.3327369689941\n",
            "Predicting forces...\n",
            "Energy model summary: \n",
            "Time invariants:  0.1900196075439453\n",
            "Time prediction:  94.63310241699219\n",
            "Time moving on device:  1.196146011352539\n",
            "Time total:  106.75287246704102\n",
            "Time for invariants (call from forces):  1583.681583404541\n",
            "Predicting forces...\n",
            "Energy model summary: \n",
            "Time invariants:  0.19121170043945312\n",
            "Time prediction:  83.96315574645996\n",
            "Time moving on device:  0.7512569427490234\n",
            "Time total:  94.06495094299316\n"
          ]
        }
      ],
      "source": [
        "forces_true = []\n",
        "forces_predicted = []\n",
        "\n",
        "energy_true = []\n",
        "energy_predicted = []\n",
        "\n",
        "for atoms in test_traj:\n",
        "    forces_true.append(atoms.get_forces())\n",
        "    energy_true.append(atoms.get_potential_energy())\n",
        "    atoms.set_calculator(fande_calc_loaded)\n",
        "    forces_predicted.append(atoms.get_forces())\n",
        "    energy_predicted.append(atoms.get_potential_energy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKCXpyfo1woX"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "energy_true = np.array(energy_true)\n",
        "energy_predicted = np.array(energy_predicted)\n",
        "energy_errors = energy_predicted - energy_true\n",
        "\n",
        "forces_true = np.array(forces_true)\n",
        "forces_predicted = np.array(forces_predicted)\n",
        "forces_errors = forces_predicted - forces_true\n",
        "\n",
        "atomic_groups = predictor_loaded.fdm.atomic_groups\n",
        "\n",
        "\n",
        "\n",
        "for ag in atomic_groups:\n",
        "\n",
        "    print(\"Atomic group\", ag)\n",
        "    print(\"F_x\")\n",
        "    print(\"MAE\", np.mean(np.abs(forces_errors[:, ag, 0].flatten())))\n",
        "    print(\"RMSE\", np.sqrt(np.mean(forces_errors[:, ag, 0].flatten()**2)))\n",
        "    print(\"F_y\")\n",
        "    print(\"MAE\", np.mean(np.abs(forces_errors[:, ag, 1].flatten())))\n",
        "    print(\"RMSE\", np.sqrt(np.mean(forces_errors[:, ag, 1].flatten()**2)))\n",
        "    print(\"F_z\")\n",
        "    print(\"MAE\", np.mean(np.abs(forces_errors[:, ag, 2].flatten())))\n",
        "    print(\"RMSE\", np.sqrt(np.mean(forces_errors[:, ag, 2].flatten()**2)))\n",
        "    print(\"E\")\n",
        "    print(\"MAE\", np.mean(np.abs(energy_errors.flatten())))\n",
        "    print(\"RMSE\", np.sqrt(np.mean(energy_errors.flatten()**2)))\n",
        "\n",
        "    fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(15, 3))\n",
        "    axs[0].set_title(r\"$F_x$\")\n",
        "    axs[0].plot(np.array(forces_true)[:, ag, 0].flatten(), label=\"true\")\n",
        "    axs[0].plot(np.array(forces_predicted)[:, ag, 0].flatten(), label=\"predicted\")\n",
        "    axs[0].legend()\n",
        "\n",
        "    axs[1].set_title(r\"$F_y$\")\n",
        "    axs[1].plot(np.array(forces_true)[:, ag, 1].flatten(), label=\"true\")\n",
        "    axs[1].plot(np.array(forces_predicted)[:, ag, 1].flatten(), label=\"predicted\")\n",
        "    axs[1].legend()\n",
        "\n",
        "    axs[2].set_title(r\"$F_z$\")\n",
        "    axs[2].plot(np.array(forces_true)[:, ag, 2].flatten(), label=\"true\")\n",
        "    axs[2].plot(np.array(forces_predicted)[:, ag, 2].flatten(), label=\"predicted\")\n",
        "    axs[2].legend()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(15, 3))\n",
        "\n",
        "    axs[0].set_title(r\"$F_x$ errors\")\n",
        "    axs[0].hist(forces_errors[:, ag, 0].flatten(), bins=100)\n",
        "\n",
        "    axs[1].set_title(r\"$F_y$ errors\")\n",
        "    axs[1].hist(forces_errors[:, ag, 1].flatten(), bins=100)\n",
        "\n",
        "    axs[2].set_title(r\"$F_z$ errors\")\n",
        "    axs[2].hist(forces_errors[:, ag, 2].flatten(), bins=100)\n",
        "    plt.show()\n",
        "\n",
        "    fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(15, 3))\n",
        "    axs[0].set_title(r\"$E$ errors\")\n",
        "    axs[0].hist(energy_errors.flatten(), bins=100)\n",
        "\n",
        "    axs[1].set_title(r\"$E$\")\n",
        "    axs[1].plot(energy_true, label=\"true\")\n",
        "    axs[1].plot(energy_predicted, label=\"predicted\")\n",
        "    axs[1].legend()\n",
        "    plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}