{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! jupyter nbconvert --to python --RegexRemovePreprocessor.patterns=\"^%\"  train_and_run.ipynb\n",
    "\n",
    "# RULES:\n",
    "# 1. Do not make plot.show() only plot.savefig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/misc/home/qklmn/repos/rotor-gp/results/train_and_run/2023-07-11_14 53-51_171350'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:info01) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss_0</td><td>█▆▄▃▂▂▂▂▂▂▂ ▂ ▂   ▅▃▃▁  ▁▁▁    ▂   ▇▅▃▂▁</td></tr><tr><td>loss_1</td><td>▇▃▂▂▂▂▁▁▁▁▁▁▁█▄▂▂▂▂▂▁▁▁▁▁▁▁▄▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>loss_2</td><td>▇▆▄▂▁▁▁▁▁▂▁▁▁█▆▄▂▁▁▁▁▁▁▂▁▂▁▇▅▂▁▁▁▁▁▁▁▂▂▁</td></tr><tr><td>loss_3</td><td>▇▅▃▁▁▁▁▁▁▁▁▁▁█▅▃▁▁▁▁▁▁▁▁▁▁▁▆▄▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss_4</td><td>▇▅▃▂▁▁▁▁▁▁▁▁▁█▅▃▂▁▁▁▁▁▁▁▁▁▁▆▄▂▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss_0</td><td>0.30013</td></tr><tr><td>loss_1</td><td>1.65398</td></tr><tr><td>loss_2</td><td>0.64738</td></tr><tr><td>loss_3</td><td>0.86066</td></tr><tr><td>loss_4</td><td>0.92807</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">info01</strong> at: <a href='https://wandb.ai/muser/rotor-gp/runs/info01' target=\"_blank\">https://wandb.ai/muser/rotor-gp/runs/info01</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/misc/home/qklmn/repos/rotor-gp/results/train_and_run/2023-07-11_14 53-51_171350/wandb/run-20230711_153047-info01/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:info01). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/misc/home/qklmn/repos/rotor-gp/results/train_and_run/2023-07-11_14 53-51_171350/wandb/run-20230711_155256-info01</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/muser/rotor-gp/runs/info01' target=\"_blank\">info01</a></strong> to <a href='https://wandb.ai/muser/rotor-gp' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/muser/rotor-gp' target=\"_blank\">https://wandb.ai/muser/rotor-gp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/muser/rotor-gp/runs/info01' target=\"_blank\">https://wandb.ai/muser/rotor-gp/runs/info01</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:39<00:00,  1.27it/s]\n",
      "100%|██████████| 20/20 [00:15<00:00,  1.26it/s]\n"
     ]
    }
   ],
   "source": [
    "# import argparse\n",
    "\n",
    "# # Construct the argument parser\n",
    "# ap = argparse.ArgumentParser()\n",
    "\n",
    "# # Add the arguments to the parser\n",
    "# ap.add_argument(\"-a\", \"--foperand\", required=True,\n",
    "#    help=\"first operand\")\n",
    "# ap.add_argument(\"-b\", \"--soperand\", required=True,\n",
    "#    help=\"second operand\")\n",
    "# args = vars(ap.parse_args())\n",
    "\n",
    "# print(args)\n",
    "\n",
    "# print(\"Starting script...\")\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# export R=0; python ./train_and_run.py 2>&1 \n",
    "\n",
    "import ase\n",
    "from ase import io\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../../fande\") \n",
    "# sys.path.append(\"..\") \n",
    "import fande\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "os.environ['VASP_PP_PATH'] = \"/home/qklmn/repos/pseudos\"\n",
    "\n",
    "def make_calc_dir():\n",
    "    if os.getcwd()[-4:] != 'code':\n",
    "        os.chdir('../../../code')\n",
    "    dir_name = f'{datetime.now().strftime(\"%Y-%m-%d_%H %M-%S_%f\")}'\n",
    "    dir_name = '../results/train_and_run/' + dir_name\n",
    "    os.makedirs(dir_name, exist_ok=True)\n",
    "    abs_dir_path = os.path.abspath(dir_name)\n",
    "    return abs_dir_path\n",
    "\n",
    "temp_dir = make_calc_dir()\n",
    "os.chdir(temp_dir)\n",
    "print(\"Saving data to directory: \", temp_dir)\n",
    "\n",
    "\n",
    "import sys\n",
    "log_file = temp_dir + '/LOG_GPU.log'\n",
    "sys.stdout = open(log_file, 'w')\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "data_folder = \"/home/qklmn/data/\"\n",
    "# data_folder = \"/data1/simulations/\"\n",
    "\n",
    "print(\"Loading training data...\")\n",
    "# traj_300 = io.read(\"/data1/simulations/datasets/rotors/different_temperatures/300/OUTCAR\", format=\"vasp-out\", index = \":\")\n",
    "# traj_600 = io.read(\"/data1/simulations/datasets/rotors/different_temperatures/600/OUTCAR\", format=\"vasp-out\", index = \":\")\n",
    "# traj_900 = io.read(\"/data1/simulations/datasets/rotors/different_temperatures/900/OUTCAR\", format=\"vasp-out\", index = \":\")\n",
    "# traj_1200 = io.read(\"/data1/simulations/datasets/rotors/different_temperatures/1200/OUTCAR\", format=\"vasp-out\", index = \":\")\n",
    "# traj_1500 = io.read(\"/data1/simulations/datasets/rotors/different_temperatures/1200/OUTCAR\", format=\"vasp-out\", index = \":\")\n",
    "# traj_1800 = io.read(\"/data1/simulations/datasets/rotors/different_temperatures/1800/OUTCAR\", format=\"vasp-out\", index = \":\")\n",
    "# traj_2100 = io.read(\"/data1/simulations/datasets/rotors/different_temperatures/2100/OUTCAR\", format=\"vasp-out\", index = \":\")\n",
    "# print(len(traj_300), len(traj_600), len(traj_900), len(traj_1200), len(traj_1500), len(traj_1800), len(traj_2100))\n",
    "\n",
    "\n",
    "traj_300 = io.read(data_folder + \"datasets/rotors/different_temperatures/300/OUTCAR\", format=\"vasp-out\", index = \":\")\n",
    "# traj_600 = io.read(data_folder + \"datasets/rotors/different_temperatures/600/OUTCAR\", format=\"vasp-out\", index = \":\")\n",
    "# traj_900 = io.read(data_folder + \"datasets/rotors/different_temperatures/900/OUTCAR\", format=\"vasp-out\", index = \":\")\n",
    "# traj_1200 = io.read(data_folder + \"datasets/rotors/different_temperatures/1200/OUTCAR\", format=\"vasp-out\", index = \":\")\n",
    "traj_1500 = io.read(data_folder + \"datasets/rotors/different_temperatures/1500/OUTCAR\", format=\"vasp-out\", index = \":\")\n",
    "# traj_1800 = io.read(data_folder + \"datasets/rotors/different_temperatures/1800/OUTCAR\", format=\"vasp-out\", index = \":\")\n",
    "traj_2100 = io.read(data_folder + \"datasets/rotors/different_temperatures/2100/OUTCAR\", format=\"vasp-out\", index = \":\")\n",
    "\n",
    "# traj_train = traj_1500[::10].copy()\n",
    "traj_train = traj_2100[::10].copy()\n",
    "# training_indices = np.sort(  np.arange(0, 500, 5) )  \n",
    "# traj_train = [traj_md[i] for i in training_indices]\n",
    "\n",
    "traj_test = traj_300[300:320].copy()\n",
    "# test_indices = np.sort(  np.random.choice(np.arange(0,92795), 200, replace=False) ) \n",
    "# test_indices = np.sort(  np.arange(400,410,1) ) \n",
    "# traj_test = [traj_md[i] for i in test_indices]\n",
    "\n",
    "\n",
    "import os\n",
    "machine_name = os.uname()[1]\n",
    "\n",
    "import wandb\n",
    "\n",
    "\n",
    "wandb.init(project=\"rotor-gp\", save_code=True, notes=\"hello\", id=machine_name)\n",
    "\n",
    "## Train data:\n",
    "energies_train = np.zeros(len(traj_train) )\n",
    "forces_train = np.zeros( (len(traj_train), len(traj_train[0]), 3 ) )\n",
    "for i, snap in enumerate(traj_train):\n",
    "    energies_train[i] = snap.get_potential_energy()\n",
    "    forces_train[i] = snap.get_forces()\n",
    "train_data = {'trajectory': traj_train, 'energies': energies_train, 'forces': forces_train}\n",
    "\n",
    "## Test data:\n",
    "energies_test = np.zeros(len(traj_test) )\n",
    "forces_test = np.zeros( (len(traj_test), len(traj_test[0]), 3 ) )\n",
    "for i, snap in enumerate(traj_test):\n",
    "    energies_test[i] = snap.get_potential_energy()\n",
    "    forces_test[i] = snap.get_forces()\n",
    "test_data = {'trajectory': traj_test, 'energies': energies_test, 'forces': forces_test}\n",
    "\n",
    "\n",
    "from fande.data import FandeDataModuleASE\n",
    "\n",
    "atoms = traj_train[0].copy()\n",
    "H_atoms = [atom.index for atom in atoms if atom.symbol == \"H\"]\n",
    "C_atoms = [atom.index for atom in atoms if atom.symbol == \"C\"]\n",
    "O_atoms = [atom.index for atom in atoms if atom.symbol == \"O\"]\n",
    "N_atoms = [atom.index for atom in atoms if atom.symbol == \"N\"]\n",
    "Si_atoms = [atom.index for atom in atoms if atom.symbol == \"Si\"]\n",
    "atomic_groups = [H_atoms, C_atoms, O_atoms, N_atoms, Si_atoms]\n",
    "\n",
    "train_centers_positions = list(range(len(atoms)))\n",
    "train_derivatives_positions = list(range(len(atoms)))\n",
    "\n",
    "# Hyperparameters:\n",
    "hparams = {}\n",
    "\n",
    "# Descriptors parameters:\n",
    "soap_params = {\n",
    "    'species': [\"H\", \"C\", \"O\", \"N\", \"Si\"],\n",
    "    'periodic': True,\n",
    "    'rcut': 4.0,\n",
    "    'sigma': 0.5,\n",
    "    'nmax': 5,\n",
    "    'lmax': 5,\n",
    "    'average': \"off\",\n",
    "    'crossover': True,\n",
    "    'dtype': \"float64\",\n",
    "    'n_jobs': 10,\n",
    "    'sparse': False,\n",
    "    'positions': [7, 11, 15] # ignored\n",
    "}\n",
    "\n",
    "fdm = FandeDataModuleASE(train_data, test_data, hparams)\n",
    "\n",
    "fdm.calculate_invariants_librascal(\n",
    "    soap_params,\n",
    "    atomic_groups = atomic_groups,\n",
    "    centers_positions = train_centers_positions, \n",
    "    derivatives_positions = train_derivatives_positions,\n",
    "    same_centers_derivatives=True,\n",
    "    frames_per_batch=1,\n",
    "    calculation_context=\"train\")\n",
    "\n",
    "fdm.calculate_invariants_librascal(\n",
    "    soap_params,\n",
    "    atomic_groups = atomic_groups,\n",
    "    centers_positions = train_centers_positions, \n",
    "    derivatives_positions = train_derivatives_positions,\n",
    "    same_centers_derivatives=True,\n",
    "    frames_per_batch=1,\n",
    "    calculation_context=\"test\")\n",
    "\n",
    "\n",
    "\n",
    "for g in range(len(atomic_groups)):\n",
    "    print(\"\\n-----------\")\n",
    "    print(\"Group \", g)\n",
    "    print(\"-----------\")\n",
    "    plt.plot(fdm.train_F[g].cpu()[1::1], linestyle = 'None', marker='o', label='train')\n",
    "    plt.plot(fdm.test_F[g].cpu()[1::1], linestyle = 'None', marker='x', label='test')\n",
    "    plt.savefig(\"tran_test_forces_group_\" + str(g) + \".png\")\n",
    "    plt.close()\n",
    "\n",
    "    plt.hist(fdm.test_F[g].cpu().numpy())\n",
    "    plt.savefig(\"histogram_forces_group_\" + str(g) + \".png\")\n",
    "    plt.close()\n",
    "    print(f\"Number of training points for group {g}: \", fdm.train_DX[g].shape[-2])\n",
    "    print(f\"Number of test points for group {g}: \", fdm.test_DX[g].shape[-2])\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "total_training_random_samples = 10\n",
    "high_force_samples = 5\n",
    "\n",
    "random_samples = total_training_random_samples - high_force_samples\n",
    "\n",
    "\n",
    "indices_high_force = torch.concat( \n",
    "    (torch.topk(fdm.test_F[0], high_force_samples//2, largest=True)[1],  \n",
    "     torch.topk(fdm.test_F[0], high_force_samples//2, largest=False)[1]) ).cpu().numpy()\n",
    "\n",
    "ind_slice = np.sort(  \n",
    "    np.random.choice(np.setdiff1d(np.arange(0, fdm.train_F[0].shape[0]), indices_high_force), random_samples, replace=False) )\n",
    "\n",
    "indices = np.concatenate((ind_slice, indices_high_force))\n",
    "indices = np.unique(indices)\n",
    "\n",
    "\n",
    "print(\"High force indices: \", indices_high_force)\n",
    "print(ind_slice)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: /misc/home/qklmn/repos/rotor-gp/results/train_and_run/2023-07-11_15 52-44_110694/lightning_logs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [5]\n",
      "\n",
      "  | Name       | Type                       | Params\n",
      "----------------------------------------------------------\n",
      "0 | likelihood | GaussianLikelihood         | 1     \n",
      "1 | model      | ExactGPModelForces         | 2.3 K \n",
      "2 | mll        | ExactMarginalLogLikelihood | 2.3 K \n",
      "----------------------------------------------------------\n",
      "2.3 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.3 K     Total params\n",
      "0.009     Total estimated model params size (MB)\n",
      "/home/qklmn/mambaforge/envs/gpuenv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning:\n",
      "\n",
      "The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 56 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\n",
      "/home/qklmn/mambaforge/envs/gpuenv/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:280: PossibleUserWarning:\n",
      "\n",
      "The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "`Trainer.fit` stopped: `max_epochs=300` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [5]\n",
      "\n",
      "  | Name       | Type                       | Params\n",
      "----------------------------------------------------------\n",
      "0 | likelihood | GaussianLikelihood         | 1     \n",
      "1 | model      | ExactGPModelForces         | 2.3 K \n",
      "2 | mll        | ExactMarginalLogLikelihood | 2.3 K \n",
      "----------------------------------------------------------\n",
      "2.3 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.3 K     Total params\n",
      "0.009     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=300` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [5]\n",
      "\n",
      "  | Name       | Type                       | Params\n",
      "----------------------------------------------------------\n",
      "0 | likelihood | GaussianLikelihood         | 1     \n",
      "1 | model      | ExactGPModelForces         | 2.3 K \n",
      "2 | mll        | ExactMarginalLogLikelihood | 2.3 K \n",
      "----------------------------------------------------------\n",
      "2.3 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.3 K     Total params\n",
      "0.009     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=300` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [5]\n",
      "\n",
      "  | Name       | Type                       | Params\n",
      "----------------------------------------------------------\n",
      "0 | likelihood | GaussianLikelihood         | 1     \n",
      "1 | model      | ExactGPModelForces         | 2.3 K \n",
      "2 | mll        | ExactMarginalLogLikelihood | 2.3 K \n",
      "----------------------------------------------------------\n",
      "2.3 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.3 K     Total params\n",
      "0.009     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=300` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [5]\n",
      "\n",
      "  | Name       | Type                       | Params\n",
      "----------------------------------------------------------\n",
      "0 | likelihood | GaussianLikelihood         | 1     \n",
      "1 | model      | ExactGPModelForces         | 2.3 K \n",
      "2 | mll        | ExactMarginalLogLikelihood | 2.3 K \n",
      "----------------------------------------------------------\n",
      "2.3 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.3 K     Total params\n",
      "0.009     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=300` reached.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from fande.models import ModelForces, GroupModelForces, ModelEnergies, MyCallbacks\n",
    "\n",
    "import numpy as np\n",
    "# seed_everything(42, workers=True)\n",
    "import torch\n",
    "\n",
    "hparams = {\n",
    "    'dtype' : 'float32',\n",
    "    'device' : 'gpu'\n",
    "}\n",
    "\n",
    "\n",
    "per_model_hparams = []\n",
    "\n",
    "train_DX = fdm.train_DX\n",
    "train_F = fdm.train_F\n",
    "test_DX = fdm.test_DX\n",
    "test_F = fdm.test_F\n",
    "\n",
    "\n",
    "model_H_hparams = {\n",
    "    'atomic_group' : H_atoms,\n",
    "    'dtype' : hparams['dtype'],\n",
    "    'device' : hparams['device'],\n",
    "    'num_epochs' : 300,\n",
    "    'learning_rate' : 0.01,\n",
    "    'soap_dim' : fdm.train_DX[0].shape[-1],\n",
    "    'soap_params' : soap_params,\n",
    "}\n",
    "\n",
    "model_C_hparams = {\n",
    "    'atomic_group' : C_atoms,\n",
    "    'dtype' : hparams['dtype'],\n",
    "    'device' : hparams['device'],\n",
    "    'num_epochs' : 300, #800 is good\n",
    "    'learning_rate' : 0.05,\n",
    "    'soap_dim' : fdm.train_DX[1].shape[-1],\n",
    "    'soap_params' : soap_params,\n",
    "}\n",
    "\n",
    "model_N_hparams = {\n",
    "    'atomic_group' : N_atoms,\n",
    "    'dtype' : hparams['dtype'],\n",
    "    'device' : hparams['device'],\n",
    "    'num_epochs' : 300, #800 is good\n",
    "    'learning_rate' : 0.05,\n",
    "    'soap_dim' : fdm.train_DX[1].shape[-1],\n",
    "    'soap_params' : soap_params,\n",
    "}\n",
    "\n",
    "model_O_hparams = {\n",
    "    'atomic_group' : O_atoms,\n",
    "    'dtype' : hparams['dtype'],\n",
    "    'device' : hparams['device'],\n",
    "    'num_epochs' : 300, #800 is good\n",
    "    'learning_rate' : 0.05,\n",
    "    'soap_dim' : fdm.train_DX[1].shape[-1],\n",
    "    'soap_params' : soap_params,\n",
    "}\n",
    "\n",
    "model_Si_hparams = {\n",
    "    'atomic_group' : Si_atoms,\n",
    "    'dtype' : hparams['dtype'],\n",
    "    'device' : hparams['device'],\n",
    "    'num_epochs' : 300, #800 is good\n",
    "    'learning_rate' : 0.05,\n",
    "    'soap_dim' : fdm.train_DX[1].shape[-1],\n",
    "    'soap_params' : soap_params,\n",
    "}\n",
    "\n",
    "\n",
    "hparams['per_model_hparams'] = [ model_H_hparams, model_C_hparams, model_N_hparams, model_O_hparams, model_Si_hparams ] # access per_model_hparams by model.model_id\n",
    "hparams['soap_dim'] = fdm.train_DX[0].shape[-1]\n",
    "\n",
    "\n",
    "### Prepare data loaders and specify how to sample data for each group:\n",
    "total_samples_per_group = [\n",
    "    1_000, #H\n",
    "    1_000, #C\n",
    "    1_000, #N\n",
    "    1_000, #O\n",
    "    1_000, #Si    \n",
    "    ]\n",
    "\n",
    "high_force_samples_per_group = [\n",
    "    1000,\n",
    "    10,\n",
    "    10,\n",
    "    10,\n",
    "    10,]\n",
    "\n",
    "train_data_loaders = fdm.prepare_train_data_loaders(\n",
    "    total_samples_per_group=total_samples_per_group,\n",
    "    high_force_samples_per_group=high_force_samples_per_group)\n",
    "hparams['train_indices'] = fdm.train_indices\n",
    "#####################################################################\n",
    "\n",
    "model_H = ModelForces(\n",
    "    train_x = train_data_loaders[0].dataset[:][0],\n",
    "    train_y = train_data_loaders[0].dataset[:][1],\n",
    "    atomic_group = H_atoms,\n",
    "    hparams = hparams,\n",
    "    id=0)\n",
    "\n",
    "model_C = ModelForces(\n",
    "    train_x = train_data_loaders[1].dataset[:][0],\n",
    "    train_y = train_data_loaders[1].dataset[:][1],\n",
    "    atomic_group = C_atoms,\n",
    "    hparams = hparams,\n",
    "    id=1)\n",
    "\n",
    "model_N = ModelForces(\n",
    "    train_x = train_data_loaders[2].dataset[:][0],\n",
    "    train_y = train_data_loaders[2].dataset[:][1],\n",
    "    atomic_group = N_atoms,\n",
    "    hparams = hparams,\n",
    "    id=2)\n",
    "\n",
    "model_O = ModelForces(\n",
    "    train_x = train_data_loaders[3].dataset[:][0],\n",
    "    train_y = train_data_loaders[3].dataset[:][1],\n",
    "    atomic_group = O_atoms,\n",
    "    hparams = hparams,\n",
    "    id=3)\n",
    "\n",
    "model_Si = ModelForces(\n",
    "    train_x = train_data_loaders[4].dataset[:][0],\n",
    "    train_y = train_data_loaders[4].dataset[:][1],\n",
    "    atomic_group = Si_atoms,\n",
    "    hparams = hparams,\n",
    "    id=4)\n",
    "\n",
    "\n",
    "AG_force_model = GroupModelForces(\n",
    "    models=[model_H, model_C, model_N, model_O, model_Si],\n",
    "    train_data_loaders = train_data_loaders,\n",
    "    hparams=hparams)\n",
    "\n",
    "AG_force_model.fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14400, 1200])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.count_nonzero(fdm.train_DX[2][100])\n",
    "\n",
    "# fdm.train_DX[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [5]\n",
      "/home/qklmn/mambaforge/envs/gpuenv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning:\n",
      "\n",
      "The dataloader, predict_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 56 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [5]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [5]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [5]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [5]\n"
     ]
    }
   ],
   "source": [
    "### TESTING PREDICITONS ###\n",
    "print('Testing performance with (meta-)dynamics run...')\n",
    "\n",
    "from fande.predict import PredictorASE\n",
    "\n",
    "model_e = None\n",
    "trainer_e = None\n",
    "\n",
    "AG_force_model.eval()\n",
    "\n",
    "predictor = PredictorASE(\n",
    "            fdm,\n",
    "            model_e,\n",
    "            trainer_e,\n",
    "            AG_force_model,\n",
    "            # trainer_f,\n",
    "            hparams,\n",
    "            soap_params\n",
    ")\n",
    "\n",
    "predictor.test_errors(view_worst_atoms=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MD with fande calc\n",
    "from fande.ase import FandeCalc\n",
    "\n",
    "\n",
    "# from ase.geometry.analysis import Analysis\n",
    "# from ase.constraints import FixAtoms, FixBondLengths\n",
    "from ase.optimize import BFGS\n",
    "from ase import units\n",
    "from ase.io import read\n",
    "import logging\n",
    "from ase.md.velocitydistribution import MaxwellBoltzmannDistribution\n",
    "from ase.md.verlet import VelocityVerlet\n",
    "from ase.md.langevin import Langevin\n",
    "from ase.md.npt import NPT\n",
    "from ase.md.nptberendsen import NPTBerendsen\n",
    "from ase.md.nvtberendsen import NVTBerendsen\n",
    "\n",
    "\n",
    "logging.getLogger(\"pytorch_lightning\").setLevel(logging.ERROR) # logging.ERROR to disable or INFO\n",
    "\n",
    "# traj_md = read('../results/test/machine_learning/dftb_opt_1000_six_rings.traj', index=\":\")\n",
    "# traj_opt = read('../results/test/machine_learning/opt.traj', index=\":\")\n",
    "\n",
    "# atoms = fdm.mol_traj[10].copy()\n",
    "# atoms = traj_md[300].copy()\n",
    "# atoms = traj_opt[-1].copy()\n",
    "atoms = traj_test[10].copy()\n",
    "atoms.set_pbc(True)\n",
    "\n",
    "\n",
    "atoms.calc = FandeCalc(predictor)\n",
    "# atoms.calc.set_atomic_groups([rings_carbons, rings_hydrogens], titles=[\"Rings carbons\", \"Rings hydrogens\"])\n",
    "# atoms.calc.set_forces_errors_plot_file(\"../results/test/md_runs/forces_errors.png\", loginterval=1)\n",
    "# atoms.calc = LennardJones()\n",
    "\n",
    "# Verlet dynamics:\n",
    "MaxwellBoltzmannDistribution(atoms, temperature_K=300)\n",
    "# dyn = VelocityVerlet(\n",
    "#     atoms,\n",
    "#     dt = 0.5*units.fs,\n",
    "#     trajectory=\"../results/test/md_runs/md_test.traj\",\n",
    "#     logfile=\"../results/test/md_runs/md_log.log\",\n",
    "# )\n",
    "\n",
    "# dyn = NPT(\n",
    "#     atoms,\n",
    "#     # dt = 0.5*units.fs,\n",
    "#     timestep=0.1,\n",
    "#     temperature_K=300,\n",
    "#     externalstress=0.0,\n",
    "#     trajectory=\"../results/test/md_runs/md_test.traj\",\n",
    "#     logfile=\"../results/test/md_runs/md_log.log\",\n",
    "# )\n",
    "\n",
    "# dyn = NPTBerendsen(atoms, timestep=0.1 * units.fs, temperature_K=300,\n",
    "#                    taut=100 * units.fs, pressure_au=1.01325 * units.bar,\n",
    "#                    taup=1000 * units.fs, compressibility=4.57e-5 / units.bar,\n",
    "#                    trajectory=\"../results/test/md_runs/md_test.traj\",\n",
    "#                    logfile=\"../results/test/md_runs/md_log.log\",)\n",
    "\n",
    "# import os\n",
    "\n",
    "os.makedirs(\"md_run/\", exist_ok=True)\n",
    "\n",
    "# dyn = NVTBerendsen(atoms, 0.5 * units.fs, 300, taut=0.5*1000*units.fs, \n",
    "#                    trajectory=\"md_run/md_test.traj\",   \n",
    "#                    logfile=\"md_run/md_log.log\")\n",
    "\n",
    "# dyn.run(100)\n",
    "\n",
    "# Langevin dynamics:\n",
    "# https://databases.fysik.dtu.dk/ase/tutorials/md/md.html\n",
    "MaxwellBoltzmannDistribution(atoms, temperature_K=300)\n",
    "dyn = Langevin(atoms, 0.1, temperature_K=0.1/units.kB, friction=0.1,\n",
    "               fixcm=True, trajectory='md_run/md_test.traj',\n",
    "               logfile=\"md_run/md_log.log\")\n",
    "dyn.run(10)\n",
    "\n",
    "# # Structure optimization:\n",
    "# dyn = BFGS(\n",
    "#     atoms,\n",
    "#     trajectory=\"../results/test/md_runs/md_test.traj\",\n",
    "#     logfile=\"../results/test/md_runs/md_log.log\",)\n",
    "# dyn.run(fmax=0.1)\n",
    "\n",
    "\n",
    "print(\" ALL JOBS WITHIN PYTHON SCRIPT ARE DONE! \")\n",
    "\n",
    "print(\"TIMING: \", time.time()-start_time, \" seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
