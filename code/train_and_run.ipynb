{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! jupyter nbconvert --to python --RegexRemovePreprocessor.patterns=\"^%\"  train_and_run.ipynb\n",
    "\n",
    "# RULES:\n",
    "# 1. Do not make plot.show() only plot.savefig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qklmn/mambaforge/envs/gpuenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fande.models module imported...\n",
      "Saving data to directory:  /misc/home/qklmn/repos/rotor-gp/results/train_and_run/2023-07-11_12 35-41_764127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Network error (ConnectTimeout), entering retry loop.\n",
      "wandb: Network error (ConnectTimeout), entering retry loop.\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import argparse\n",
    "\n",
    "# # Construct the argument parser\n",
    "# ap = argparse.ArgumentParser()\n",
    "\n",
    "# # Add the arguments to the parser\n",
    "# ap.add_argument(\"-a\", \"--foperand\", required=True,\n",
    "#    help=\"first operand\")\n",
    "# ap.add_argument(\"-b\", \"--soperand\", required=True,\n",
    "#    help=\"second operand\")\n",
    "# args = vars(ap.parse_args())\n",
    "\n",
    "# print(args)\n",
    "\n",
    "# print(\"Starting script...\")\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# export R=0; python ./train_and_run.py 2>&1 \n",
    "\n",
    "import ase\n",
    "from ase import io\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../../fande\") \n",
    "# sys.path.append(\"..\") \n",
    "import fande\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "os.environ['VASP_PP_PATH'] = \"/home/qklmn/repos/pseudos\"\n",
    "\n",
    "def make_calc_dir():\n",
    "    dir_name = f'{datetime.now().strftime(\"%Y-%m-%d_%H %M-%S_%f\")}'\n",
    "    dir_name = '../results/train_and_run/' + dir_name\n",
    "    os.makedirs(dir_name, exist_ok=True)\n",
    "    abs_dir_path = os.path.abspath(dir_name)\n",
    "    return abs_dir_path\n",
    "\n",
    "temp_dir = make_calc_dir()\n",
    "os.chdir(temp_dir)\n",
    "print(\"Saving data to directory: \", temp_dir)\n",
    "\n",
    "\n",
    "import sys\n",
    "log_file = temp_dir + '/LOG_GPU.log'\n",
    "sys.stdout = open(log_file, 'w')\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "data_folder = \"/home/qklmn/data/\"\n",
    "# data_folder = \"/data1/simulations/\"\n",
    "\n",
    "print(\"Loading training data...\")\n",
    "# traj_300 = io.read(\"/data1/simulations/datasets/rotors/different_temperatures/300/OUTCAR\", format=\"vasp-out\", index = \":\")\n",
    "# traj_600 = io.read(\"/data1/simulations/datasets/rotors/different_temperatures/600/OUTCAR\", format=\"vasp-out\", index = \":\")\n",
    "# traj_900 = io.read(\"/data1/simulations/datasets/rotors/different_temperatures/900/OUTCAR\", format=\"vasp-out\", index = \":\")\n",
    "# traj_1200 = io.read(\"/data1/simulations/datasets/rotors/different_temperatures/1200/OUTCAR\", format=\"vasp-out\", index = \":\")\n",
    "# traj_1500 = io.read(\"/data1/simulations/datasets/rotors/different_temperatures/1200/OUTCAR\", format=\"vasp-out\", index = \":\")\n",
    "# traj_1800 = io.read(\"/data1/simulations/datasets/rotors/different_temperatures/1800/OUTCAR\", format=\"vasp-out\", index = \":\")\n",
    "# traj_2100 = io.read(\"/data1/simulations/datasets/rotors/different_temperatures/2100/OUTCAR\", format=\"vasp-out\", index = \":\")\n",
    "# print(len(traj_300), len(traj_600), len(traj_900), len(traj_1200), len(traj_1500), len(traj_1800), len(traj_2100))\n",
    "\n",
    "\n",
    "traj_300 = io.read(data_folder + \"datasets/rotors/different_temperatures/300/OUTCAR\", format=\"vasp-out\", index = \":\")\n",
    "# traj_600 = io.read(data_folder + \"datasets/rotors/different_temperatures/600/OUTCAR\", format=\"vasp-out\", index = \":\")\n",
    "# traj_900 = io.read(data_folder + \"datasets/rotors/different_temperatures/900/OUTCAR\", format=\"vasp-out\", index = \":\")\n",
    "# traj_1200 = io.read(data_folder + \"datasets/rotors/different_temperatures/1200/OUTCAR\", format=\"vasp-out\", index = \":\")\n",
    "traj_1500 = io.read(data_folder + \"datasets/rotors/different_temperatures/1500/OUTCAR\", format=\"vasp-out\", index = \":\")\n",
    "# traj_1800 = io.read(data_folder + \"datasets/rotors/different_temperatures/1800/OUTCAR\", format=\"vasp-out\", index = \":\")\n",
    "# traj_2100 = io.read(data_folder + \"datasets/rotors/different_temperatures/2100/OUTCAR\", format=\"vasp-out\", index = \":\")\n",
    "\n",
    "traj_train = traj_1500[::10].copy()\n",
    "# training_indices = np.sort(  np.arange(0, 500, 5) )  \n",
    "# traj_train = [traj_md[i] for i in training_indices]\n",
    "\n",
    "traj_test = traj_300[300:320].copy()\n",
    "# test_indices = np.sort(  np.random.choice(np.arange(0,92795), 200, replace=False) ) \n",
    "# test_indices = np.sort(  np.arange(400,410,1) ) \n",
    "# traj_test = [traj_md[i] for i in test_indices]\n",
    "\n",
    "\n",
    "import os\n",
    "machine_name = os.uname()[1]\n",
    "\n",
    "import wandb\n",
    "\n",
    "\n",
    "wandb.init(project=\"rotor-gp\", save_code=True, notes=\"hello\", id=machine_name, mode=\"online\")\n",
    "\n",
    "## Train data:\n",
    "energies_train = np.zeros(len(traj_train) )\n",
    "forces_train = np.zeros( (len(traj_train), len(traj_train[0]), 3 ) )\n",
    "for i, snap in enumerate(traj_train):\n",
    "    energies_train[i] = snap.get_potential_energy()\n",
    "    forces_train[i] = snap.get_forces()\n",
    "train_data = {'trajectory': traj_train, 'energies': energies_train, 'forces': forces_train}\n",
    "\n",
    "## Test data:\n",
    "energies_test = np.zeros(len(traj_test) )\n",
    "forces_test = np.zeros( (len(traj_test), len(traj_test[0]), 3 ) )\n",
    "for i, snap in enumerate(traj_test):\n",
    "    energies_test[i] = snap.get_potential_energy()\n",
    "    forces_test[i] = snap.get_forces()\n",
    "test_data = {'trajectory': traj_test, 'energies': energies_test, 'forces': forces_test}\n",
    "\n",
    "\n",
    "from fande.data import FandeDataModuleASE\n",
    "\n",
    "atoms = traj_train[0].copy()\n",
    "H_atoms = [atom.index for atom in atoms if atom.symbol == \"H\"]\n",
    "C_atoms = [atom.index for atom in atoms if atom.symbol == \"C\"]\n",
    "O_atoms = [atom.index for atom in atoms if atom.symbol == \"O\"]\n",
    "N_atoms = [atom.index for atom in atoms if atom.symbol == \"N\"]\n",
    "Si_atoms = [atom.index for atom in atoms if atom.symbol == \"Si\"]\n",
    "atomic_groups = [H_atoms, C_atoms, O_atoms, N_atoms, Si_atoms]\n",
    "\n",
    "train_centers_positions = list(range(len(atoms)))\n",
    "train_derivatives_positions = list(range(len(atoms)))\n",
    "\n",
    "# Hyperparameters:\n",
    "hparams = {}\n",
    "\n",
    "# Descriptors parameters:\n",
    "soap_params = {\n",
    "    'species': [\"H\", \"C\", \"O\", \"N\", \"Si\"],\n",
    "    'periodic': True,\n",
    "    'rcut': 4.0,\n",
    "    'sigma': 0.5,\n",
    "    'nmax': 4,\n",
    "    'lmax': 4,\n",
    "    'average': \"off\",\n",
    "    'crossover': True,\n",
    "    'dtype': \"float64\",\n",
    "    'n_jobs': 10,\n",
    "    'sparse': False,\n",
    "    'positions': [7, 11, 15] # ignored\n",
    "}\n",
    "\n",
    "fdm = FandeDataModuleASE(train_data, test_data, hparams)\n",
    "\n",
    "fdm.calculate_invariants_librascal(\n",
    "    soap_params,\n",
    "    atomic_groups = atomic_groups,\n",
    "    centers_positions = train_centers_positions, \n",
    "    derivatives_positions = train_derivatives_positions,\n",
    "    same_centers_derivatives=True,\n",
    "    frames_per_batch=1,\n",
    "    calculation_context=\"train\")\n",
    "\n",
    "fdm.calculate_invariants_librascal(\n",
    "    soap_params,\n",
    "    atomic_groups = atomic_groups,\n",
    "    centers_positions = train_centers_positions, \n",
    "    derivatives_positions = train_derivatives_positions,\n",
    "    same_centers_derivatives=True,\n",
    "    frames_per_batch=1,\n",
    "    calculation_context=\"test\")\n",
    "\n",
    "\n",
    "\n",
    "for g in range(len(atomic_groups)):\n",
    "    print(\"\\n-----------\")\n",
    "    print(\"Group \", g)\n",
    "    print(\"-----------\")\n",
    "    plt.plot(fdm.train_F[g].cpu()[1::1], linestyle = 'None', marker='o', label='train')\n",
    "    plt.plot(fdm.test_F[g].cpu()[1::1], linestyle = 'None', marker='x', label='test')\n",
    "    plt.savefig(\"tran_test_forces_group_\" + str(g) + \".png\")\n",
    "    plt.close()\n",
    "\n",
    "    plt.hist(fdm.test_F[g].cpu().numpy())\n",
    "    plt.savefig(\"histogram_forces_group_\" + str(g) + \".png\")\n",
    "    plt.close()\n",
    "    print(f\"Number of training points for group {g}: \", fdm.train_DX[g].shape[-2])\n",
    "    print(f\"Number of test points for group {g}: \", fdm.test_DX[g].shape[-2])\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "total_training_random_samples = 10\n",
    "high_force_samples = 5\n",
    "\n",
    "random_samples = total_training_random_samples - high_force_samples\n",
    "\n",
    "\n",
    "indices_high_force = torch.concat( \n",
    "    (torch.topk(fdm.test_F[0], high_force_samples//2, largest=True)[1],  \n",
    "     torch.topk(fdm.test_F[0], high_force_samples//2, largest=False)[1]) ).cpu().numpy()\n",
    "\n",
    "ind_slice = np.sort(  \n",
    "    np.random.choice(np.setdiff1d(np.arange(0, fdm.train_F[0].shape[0]), indices_high_force), random_samples, replace=False) )\n",
    "\n",
    "indices = np.concatenate((ind_slice, indices_high_force))\n",
    "indices = np.unique(indices)\n",
    "\n",
    "\n",
    "print(\"High force indices: \", indices_high_force)\n",
    "print(ind_slice)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: /misc/home/qklmn/repos/rotor-gp/results/train_and_run/2023-07-11_12 25-07_290954/lightning_logs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [5]\n",
      "\n",
      "  | Name       | Type                       | Params\n",
      "----------------------------------------------------------\n",
      "0 | likelihood | GaussianLikelihood         | 1     \n",
      "1 | model      | ExactGPModelForces         | 1.2 K \n",
      "2 | mll        | ExactMarginalLogLikelihood | 1.2 K \n",
      "----------------------------------------------------------\n",
      "1.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.2 K     Total params\n",
      "0.005     Total estimated model params size (MB)\n",
      "/home/qklmn/mambaforge/envs/gpuenv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning:\n",
      "\n",
      "The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 56 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\n",
      "/home/qklmn/mambaforge/envs/gpuenv/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:280: PossibleUserWarning:\n",
      "\n",
      "The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "`Trainer.fit` stopped: `max_epochs=600` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [5]\n",
      "\n",
      "  | Name       | Type                       | Params\n",
      "----------------------------------------------------------\n",
      "0 | likelihood | GaussianLikelihood         | 1     \n",
      "1 | model      | ExactGPModelForces         | 1.2 K \n",
      "2 | mll        | ExactMarginalLogLikelihood | 1.2 K \n",
      "----------------------------------------------------------\n",
      "1.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.2 K     Total params\n",
      "0.005     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=300` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [5]\n",
      "\n",
      "  | Name       | Type                       | Params\n",
      "----------------------------------------------------------\n",
      "0 | likelihood | GaussianLikelihood         | 1     \n",
      "1 | model      | ExactGPModelForces         | 1.2 K \n",
      "2 | mll        | ExactMarginalLogLikelihood | 1.2 K \n",
      "----------------------------------------------------------\n",
      "1.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.2 K     Total params\n",
      "0.005     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=300` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [5]\n",
      "\n",
      "  | Name       | Type                       | Params\n",
      "----------------------------------------------------------\n",
      "0 | likelihood | GaussianLikelihood         | 1     \n",
      "1 | model      | ExactGPModelForces         | 1.2 K \n",
      "2 | mll        | ExactMarginalLogLikelihood | 1.2 K \n",
      "----------------------------------------------------------\n",
      "1.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.2 K     Total params\n",
      "0.005     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=300` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [5]\n",
      "\n",
      "  | Name       | Type                       | Params\n",
      "----------------------------------------------------------\n",
      "0 | likelihood | GaussianLikelihood         | 1     \n",
      "1 | model      | ExactGPModelForces         | 1.2 K \n",
      "2 | mll        | ExactMarginalLogLikelihood | 1.2 K \n",
      "----------------------------------------------------------\n",
      "1.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.2 K     Total params\n",
      "0.005     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=300` reached.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from fande.models import ModelForces, GroupModelForces, ModelEnergies, MyCallbacks\n",
    "\n",
    "import numpy as np\n",
    "# seed_everything(42, workers=True)\n",
    "import torch\n",
    "\n",
    "hparams = {\n",
    "    'dtype' : 'float32',\n",
    "    'device' : 'gpu'\n",
    "}\n",
    "\n",
    "\n",
    "per_model_hparams = []\n",
    "\n",
    "train_DX = fdm.train_DX\n",
    "train_F = fdm.train_F\n",
    "test_DX = fdm.test_DX\n",
    "test_F = fdm.test_F\n",
    "\n",
    "\n",
    "model_H_hparams = {\n",
    "    'atomic_group' : H_atoms,\n",
    "    'dtype' : hparams['dtype'],\n",
    "    'device' : hparams['device'],\n",
    "    'num_epochs' : 600,\n",
    "    'learning_rate' : 0.01,\n",
    "    'soap_dim' : fdm.train_DX[0].shape[-1],\n",
    "    'soap_params' : soap_params,\n",
    "}\n",
    "\n",
    "model_C_hparams = {\n",
    "    'atomic_group' : C_atoms,\n",
    "    'dtype' : hparams['dtype'],\n",
    "    'device' : hparams['device'],\n",
    "    'num_epochs' : 300, #800 is good\n",
    "    'learning_rate' : 0.05,\n",
    "    'soap_dim' : fdm.train_DX[1].shape[-1],\n",
    "    'soap_params' : soap_params,\n",
    "}\n",
    "\n",
    "model_N_hparams = {\n",
    "    'atomic_group' : N_atoms,\n",
    "    'dtype' : hparams['dtype'],\n",
    "    'device' : hparams['device'],\n",
    "    'num_epochs' : 300, #800 is good\n",
    "    'learning_rate' : 0.05,\n",
    "    'soap_dim' : fdm.train_DX[1].shape[-1],\n",
    "    'soap_params' : soap_params,\n",
    "}\n",
    "\n",
    "model_O_hparams = {\n",
    "    'atomic_group' : O_atoms,\n",
    "    'dtype' : hparams['dtype'],\n",
    "    'device' : hparams['device'],\n",
    "    'num_epochs' : 300, #800 is good\n",
    "    'learning_rate' : 0.05,\n",
    "    'soap_dim' : fdm.train_DX[1].shape[-1],\n",
    "    'soap_params' : soap_params,\n",
    "}\n",
    "\n",
    "model_Si_hparams = {\n",
    "    'atomic_group' : Si_atoms,\n",
    "    'dtype' : hparams['dtype'],\n",
    "    'device' : hparams['device'],\n",
    "    'num_epochs' : 300, #800 is good\n",
    "    'learning_rate' : 0.05,\n",
    "    'soap_dim' : fdm.train_DX[1].shape[-1],\n",
    "    'soap_params' : soap_params,\n",
    "}\n",
    "\n",
    "\n",
    "hparams['per_model_hparams'] = [ model_H_hparams, model_C_hparams, model_N_hparams, model_O_hparams, model_Si_hparams ] # access per_model_hparams by model.model_id\n",
    "hparams['soap_dim'] = fdm.train_DX[0].shape[-1]\n",
    "\n",
    "\n",
    "### Prepare data loaders and specify how to sample data for each group:\n",
    "total_samples_per_group = [\n",
    "    1_000, #H\n",
    "    1_000, #C\n",
    "    300, #N\n",
    "    300, #O\n",
    "    300, #Si    \n",
    "    ]\n",
    "\n",
    "high_force_samples_per_group = [\n",
    "    100,\n",
    "    10,\n",
    "    10,\n",
    "    10,\n",
    "    10,]\n",
    "\n",
    "train_data_loaders = fdm.prepare_train_data_loaders(\n",
    "    total_samples_per_group=total_samples_per_group,\n",
    "    high_force_samples_per_group=high_force_samples_per_group)\n",
    "hparams['train_indices'] = fdm.train_indices\n",
    "#####################################################################\n",
    "\n",
    "model_H = ModelForces(\n",
    "    train_x = train_data_loaders[0].dataset[:][0],\n",
    "    train_y = train_data_loaders[0].dataset[:][1],\n",
    "    atomic_group = H_atoms,\n",
    "    hparams = hparams,\n",
    "    id=0)\n",
    "\n",
    "model_C = ModelForces(\n",
    "    train_x = train_data_loaders[1].dataset[:][0],\n",
    "    train_y = train_data_loaders[1].dataset[:][1],\n",
    "    atomic_group = C_atoms,\n",
    "    hparams = hparams,\n",
    "    id=1)\n",
    "\n",
    "model_N = ModelForces(\n",
    "    train_x = train_data_loaders[2].dataset[:][0],\n",
    "    train_y = train_data_loaders[2].dataset[:][1],\n",
    "    atomic_group = N_atoms,\n",
    "    hparams = hparams,\n",
    "    id=2)\n",
    "\n",
    "model_O = ModelForces(\n",
    "    train_x = train_data_loaders[3].dataset[:][0],\n",
    "    train_y = train_data_loaders[3].dataset[:][1],\n",
    "    atomic_group = O_atoms,\n",
    "    hparams = hparams,\n",
    "    id=3)\n",
    "\n",
    "model_Si = ModelForces(\n",
    "    train_x = train_data_loaders[4].dataset[:][0],\n",
    "    train_y = train_data_loaders[4].dataset[:][1],\n",
    "    atomic_group = Si_atoms,\n",
    "    hparams = hparams,\n",
    "    id=4)\n",
    "\n",
    "\n",
    "AG_force_model = GroupModelForces(\n",
    "    models=[model_H, model_C, model_N, model_O, model_Si],\n",
    "    train_data_loaders = train_data_loaders,\n",
    "    hparams=hparams)\n",
    "\n",
    "AG_force_model.fit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TESTING PREDICITONS ###\n",
    "print('Testing performance with (meta-)dynamics run...')\n",
    "\n",
    "from fande.predict import PredictorASE\n",
    "\n",
    "model_e = None\n",
    "trainer_e = None\n",
    "\n",
    "AG_force_model.eval()\n",
    "\n",
    "predictor = PredictorASE(\n",
    "            fdm,\n",
    "            model_e,\n",
    "            trainer_e,\n",
    "            AG_force_model,\n",
    "            # trainer_f,\n",
    "            hparams,\n",
    "            soap_params\n",
    ")\n",
    "\n",
    "predictor.test_errors(plot=True, view_worst_atoms=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MD with fande calc\n",
    "from fande.ase import FandeCalc\n",
    "\n",
    "\n",
    "# from ase.geometry.analysis import Analysis\n",
    "# from ase.constraints import FixAtoms, FixBondLengths\n",
    "from ase.optimize import BFGS\n",
    "from ase import units\n",
    "from ase.io import read\n",
    "import logging\n",
    "from ase.md.velocitydistribution import MaxwellBoltzmannDistribution\n",
    "from ase.md.verlet import VelocityVerlet\n",
    "from ase.md.langevin import Langevin\n",
    "from ase.md.npt import NPT\n",
    "from ase.md.nptberendsen import NPTBerendsen\n",
    "from ase.md.nvtberendsen import NVTBerendsen\n",
    "\n",
    "\n",
    "logging.getLogger(\"pytorch_lightning\").setLevel(logging.ERROR) # logging.ERROR to disable or INFO\n",
    "\n",
    "# traj_md = read('../results/test/machine_learning/dftb_opt_1000_six_rings.traj', index=\":\")\n",
    "# traj_opt = read('../results/test/machine_learning/opt.traj', index=\":\")\n",
    "\n",
    "# atoms = fdm.mol_traj[10].copy()\n",
    "# atoms = traj_md[300].copy()\n",
    "# atoms = traj_opt[-1].copy()\n",
    "atoms = traj_test[10].copy()\n",
    "atoms.set_pbc(True)\n",
    "\n",
    "\n",
    "atoms.calc = FandeCalc(predictor)\n",
    "# atoms.calc.set_atomic_groups([rings_carbons, rings_hydrogens], titles=[\"Rings carbons\", \"Rings hydrogens\"])\n",
    "# atoms.calc.set_forces_errors_plot_file(\"../results/test/md_runs/forces_errors.png\", loginterval=1)\n",
    "# atoms.calc = LennardJones()\n",
    "\n",
    "# Verlet dynamics:\n",
    "MaxwellBoltzmannDistribution(atoms, temperature_K=300)\n",
    "# dyn = VelocityVerlet(\n",
    "#     atoms,\n",
    "#     dt = 0.5*units.fs,\n",
    "#     trajectory=\"../results/test/md_runs/md_test.traj\",\n",
    "#     logfile=\"../results/test/md_runs/md_log.log\",\n",
    "# )\n",
    "\n",
    "# dyn = NPT(\n",
    "#     atoms,\n",
    "#     # dt = 0.5*units.fs,\n",
    "#     timestep=0.1,\n",
    "#     temperature_K=300,\n",
    "#     externalstress=0.0,\n",
    "#     trajectory=\"../results/test/md_runs/md_test.traj\",\n",
    "#     logfile=\"../results/test/md_runs/md_log.log\",\n",
    "# )\n",
    "\n",
    "# dyn = NPTBerendsen(atoms, timestep=0.1 * units.fs, temperature_K=300,\n",
    "#                    taut=100 * units.fs, pressure_au=1.01325 * units.bar,\n",
    "#                    taup=1000 * units.fs, compressibility=4.57e-5 / units.bar,\n",
    "#                    trajectory=\"../results/test/md_runs/md_test.traj\",\n",
    "#                    logfile=\"../results/test/md_runs/md_log.log\",)\n",
    "\n",
    "# import os\n",
    "\n",
    "os.makedirs(\"md_run/\", exist_ok=True)\n",
    "\n",
    "# dyn = NVTBerendsen(atoms, 0.5 * units.fs, 300, taut=0.5*1000*units.fs, \n",
    "#                    trajectory=\"md_run/md_test.traj\",   \n",
    "#                    logfile=\"md_run/md_log.log\")\n",
    "\n",
    "# dyn.run(100)\n",
    "\n",
    "# Langevin dynamics:\n",
    "# https://databases.fysik.dtu.dk/ase/tutorials/md/md.html\n",
    "MaxwellBoltzmannDistribution(atoms, temperature_K=300)\n",
    "dyn = Langevin(atoms, 0.1, temperature_K=0.1/units.kB, friction=0.1,\n",
    "               fixcm=True, trajectory='md_run/md_test.traj',\n",
    "               logfile=\"md_run/md_log.log\")\n",
    "dyn.run(10)\n",
    "\n",
    "# # Structure optimization:\n",
    "# dyn = BFGS(\n",
    "#     atoms,\n",
    "#     trajectory=\"../results/test/md_runs/md_test.traj\",\n",
    "#     logfile=\"../results/test/md_runs/md_log.log\",)\n",
    "# dyn.run(fmax=0.1)\n",
    "\n",
    "\n",
    "print(\" ALL JOBS WITHIN PYTHON SCRIPT ARE DONE! \")\n",
    "\n",
    "print(\"TIMING: \", time.time()-start_time, \" seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
