{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit energies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rascal.representations import SphericalInvariants\n",
    "\n",
    "soap_params = {\n",
    "# 'species': [\"H\", \"C\", \"O\", \"N\", \"Si\"],\n",
    "# 'periodic': True,\n",
    "'interaction_cutoff': 3.5, #5\n",
    "'gaussian_sigma_constant': 0.3,\n",
    "'max_radial': 4, #5\n",
    "'max_angular': 4,#5\n",
    "'cutoff_smooth_width': 0.1,\n",
    "# 'average': \"off\",\n",
    "# 'crossover': True,\n",
    "# 'dtype': \"float64\",\n",
    "# 'n_jobs': 10,\n",
    "# 'sparse': False,\n",
    "# 'positions': [7, 11, 15] # ignored\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ase import io\n",
    "\n",
    "traj_295 = io.read(\"/data1/simulations/datasets/rotors/high_temp_ML_training_data/results_triasine_ML_2000/struct_295_295K/md_trajectory.traj\", index=\":\")\n",
    "# traj_355 = io.read(\"/data1/simulations/datasets/rotors/high_temp_ML_training_data/results_triasine_ML_2000/struct_355_355K/md_trajectory.traj\", index=\":\")\n",
    "\n",
    "# traj_295_2000K = io.read(\"/data1/simulations/datasets/rotors/high_temp_ML_training_data/results_triasine_ML_2000/struct_295_2000K/md_trajectory.traj\", index=\":\")\n",
    "# traj_355_2000K = io.read(\"/data1/simulations/datasets/rotors/high_temp_ML_training_data/results_triasine_ML_2000/struct_355_2000K/md_trajectory.traj\", index=\":\")\n",
    "# traj_295_2000K_forced = io.read(\"/data1/simulations/datasets/rotors/high_temp_ML_training_data/results_triasine_ML_2000/struct_295_2000K_0075force/md_trajectory.traj\", index=\":\")\n",
    "# traj_355_2000K_forced = io.read(\"/data1/simulations/datasets/rotors/high_temp_ML_training_data/results_triasine_ML_2000/struct_355_2000K_0075force/md_trajectory.traj\", index=\":\")\n",
    "\n",
    "traj = traj_295[1000:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energies = [s.get_potential_energy() for s in traj]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(energies)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "def prepare_batches(\n",
    "        # self, \n",
    "        traj, \n",
    "        forces,\n",
    "        energies, \n",
    "        frames_per_batch=10):\n",
    "        \n",
    "        n_frames = len(traj)\n",
    "        n_batches = math.ceil(n_frames/frames_per_batch)\n",
    "\n",
    "        print(f\"Total number of frames is {n_frames}\")\n",
    "        print(f\"Total number of batches is {n_batches}\")\n",
    "\n",
    "        batches = []\n",
    "        for i in range(n_batches):\n",
    "            batch = {}\n",
    "            batch[\"traj\"] = traj[i*frames_per_batch:(i+1)*frames_per_batch]\n",
    "        #     batch[\"forces\"] = forces[i*frames_per_batch:(i+1)*frames_per_batch]\n",
    "            batch[\"energies\"] = energies[i*frames_per_batch:(i+1)*frames_per_batch]\n",
    "            batches.append(batch)\n",
    "\n",
    "        return batches\n",
    "\n",
    "\n",
    "\n",
    "# species= soap_params['species']\n",
    "# periodic= soap_params['periodic']\n",
    "interaction_cutoff = soap_params['interaction_cutoff']\n",
    "gaussian_sigma_constant= soap_params['gaussian_sigma_constant']\n",
    "max_radial= soap_params['max_radial']\n",
    "max_angular= soap_params['max_angular']\n",
    "cutoff_smooth_width = soap_params['cutoff_smooth_width']\n",
    "# average= soap_params['average']\n",
    "# crossover= soap_params['crossover']\n",
    "# dtype= soap_params['dtype']\n",
    "# sparse= soap_params['sparse']\n",
    "# positions = soap_params['positions']\n",
    "\n",
    "hypers = dict(soap_type=\"PowerSpectrum\",\n",
    "        interaction_cutoff=interaction_cutoff,\n",
    "        max_radial=max_radial,\n",
    "        max_angular=max_angular,\n",
    "        gaussian_sigma_constant=gaussian_sigma_constant,\n",
    "        gaussian_sigma_type=\"Constant\",\n",
    "        cutoff_function_type=\"RadialScaling\",\n",
    "        cutoff_smooth_width=cutoff_smooth_width, # 0.1 is way better than 0.5\n",
    "        cutoff_function_parameters=\n",
    "                dict(\n",
    "                        rate=1,\n",
    "                        scale=3.5,\n",
    "                        exponent=4\n",
    "                        ),\n",
    "        radial_basis=\"GTO\",\n",
    "        normalize=False, # setting False makes model untrainable\n",
    "        #   optimization=\n",
    "        #         dict(\n",
    "        #                 Spline=dict(\n",
    "        #                    accuracy=1.0e-05\n",
    "        #                 )\n",
    "        #             ),\n",
    "        compute_gradients=True,\n",
    "        expansion_by_species_method='structure wise'\n",
    "        )\n",
    "\n",
    "for f in traj:\n",
    "        f.wrap(eps=1e-18)\n",
    "\n",
    "n_atoms = len(traj[0])\n",
    "\n",
    "frames_batches = prepare_batches(traj, [0]*len(traj), energies, frames_per_batch=1)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Total length of traj is {len(traj)}\")\n",
    "print(f\"Total number of batches {len(frames_batches)}\")       \n",
    "print(\"Calculating invariants on trajectory with librascal...\")\n",
    "\n",
    "soap = SphericalInvariants(**hypers)\n",
    "\n",
    "X_np_batched = []\n",
    "E_np_batched = []\n",
    "\n",
    "# DX_np_batched = [[] * len(frames_batches) for i in range(n_atomic_groups)]  \n",
    "# F_np_batched = [[] * len(frames_batches) for i in range(n_atomic_groups)]\n",
    "# grad_info_sub_batched = [[] * len(frames_batches) for i in range(n_atomic_groups)]\n",
    "\n",
    "for ind_b, batch in enumerate(tqdm(frames_batches)):\n",
    "        traj_b = batch['traj']\n",
    "        # forces_b = batch['forces']\n",
    "        energies_b = batch['energies']\n",
    "\n",
    "        managers = soap.transform(traj_b)\n",
    "        soap_array = managers.get_features(soap)\n",
    "        X_np_batched.append(soap_array)\n",
    "        E_np_batched.append(energies_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_non_H = []\n",
    "\n",
    "for ind,s in enumerate(traj[0].get_chemical_symbols()):\n",
    "        # print(s)\n",
    "        if s != \"H\":\n",
    "                indices_non_H.append(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_np_batched = np.array(X_np_batched)\n",
    "E_np_batched = np.array(E_np_batched).flatten()\n",
    "\n",
    "E_np_batched_no_mean = E_np_batched - np.mean(E_np_batched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 528, 1200])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "X_np_batched_stacked = X_np_batched.reshape(-1, X_np_batched.shape[-1]*X_np_batched.shape[-2])\n",
    "\n",
    "train_X = torch.tensor(X_np_batched)\n",
    "# train_X = train_X.sum(axis=1)\n",
    "train_Y = torch.tensor(E_np_batched_no_mean)\n",
    "\n",
    "\n",
    "\n",
    "test_X = train_X[0::100]\n",
    "test_Y = train_Y[0::100]\n",
    "\n",
    "train_X = train_X[0::25]\n",
    "train_Y = train_Y[0::25]\n",
    "\n",
    "print(train_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X.size(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([21120, 1200])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 554,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1 flatten the per snapshot collection of invariants\n",
    "c = train_X.reshape(-1, train_X.size(2))\n",
    "# test_X.shape\n",
    "print(c.shape)\n",
    "\n",
    "# 2. check that the flattened invariants are the same as the original invariants\n",
    "i = 10\n",
    "(train_X[i] == c[i*train_X.size(1):(i+1)*train_X.size(1)]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 21120, 21120])\n"
     ]
    }
   ],
   "source": [
    "# 3. example kernel matrix\n",
    "kern = c@c.T\n",
    "kern = kern.unsqueeze(0)\n",
    "\n",
    "print(kern.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "21120/528\n",
    "# train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "40*528"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 40, 40])\n"
     ]
    }
   ],
   "source": [
    "# https://pytorch.org/docs/stable/generated/torch.nn.AvgPool2d.html\n",
    "# https://stackoverflow.com/questions/63240702/torch-sum-subsets-of-tensor\n",
    "import torch, torch.nn as nn\n",
    "\n",
    "# pool of square window of size=3, stride=2\n",
    "m = nn.AvgPool2d(528, stride=528)\n",
    "# pool of non-square window\n",
    "# m = nn.AvgPool2d((3, 3), stride=(2, 1))\n",
    "# input = torch.randn(1, 528, 528)\n",
    "input = kern\n",
    "output = m(input)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.modules.pooling.AvgPool1d"
      ]
     },
     "execution_count": 575,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.AvgPool1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. perform block summation of the kernel matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gpytorch\n",
    "import numpy as np\n",
    "\n",
    "from gpytorch.kernels import (\n",
    "    RBFKernel,\n",
    "    ScaleKernel,\n",
    "    LinearKernel,\n",
    "    AdditiveKernel,\n",
    "    MultitaskKernel,\n",
    "    PolynomialKernel,\n",
    ")\n",
    "from gpytorch.means import ZeroMean, ConstantMean\n",
    "\n",
    "from gpytorch.models import ExactGP\n",
    "\n",
    "import torch, torch.nn as nn\n",
    "\n",
    "\n",
    "# https://stackoverflow.com/questions/76251549/setting-the-task-covariance-matrix-to-the-correlation-matrix-in-gpytorch\n",
    "\n",
    "class ExactGPModelEnergies(ExactGP):\n",
    "    def __init__(self, train_X, train_Y, likelihood):\n",
    "        super().__init__(\n",
    "            train_X, train_Y, likelihood\n",
    "        )  # the old-style super(ExactGPModel, self) was causing error!\n",
    "        self.mean_module = ConstantMean()\n",
    "\n",
    "        self.soap_dim = train_X.shape[-1]\n",
    "        self.covar_module = ScaleKernel(RBFKernel(ard_num_dims=self.soap_dim))#LinearKernel()\n",
    "\n",
    "    def forward(self, X):\n",
    "        x = X\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ExactGPModelEnergiesMulti(ExactGP):\n",
    "    def __init__(self, train_X, train_Y, likelihood):\n",
    "        super().__init__(\n",
    "            train_X, train_Y, likelihood\n",
    "        )  # the old-style super(ExactGPModel, self) was causing error!\n",
    "\n",
    "        # for i in range(528):\n",
    "        #     means.append(ConstantMean())\n",
    "\n",
    "        # alphas = torch.zeros(528)\n",
    "\n",
    "        # self.mean_module = [ZeroMean() for i in range(528)]\n",
    "        # self.covar_module = [ScaleKernel(RBFKernel(ard_num_dims=train_X.shape[-1])) for i in range(528)]\n",
    "\n",
    "        self.mean_module = ConstantMean()\n",
    "\n",
    "        self.soap_dim = train_X.shape[-1]\n",
    "        self.covar_module = ScaleKernel(RBFKernel(ard_num_dims=self.soap_dim))#LinearKernel()\n",
    "\n",
    "        self.pooler_2d = nn.AvgPool2d(528, stride=528)\n",
    "        self.pooler_1d = nn.AvgPool1d(528, stride=528)\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        x = X.reshape(-1, X.size(2))\n",
    "        \n",
    "        mean_full = self.mean_module(x)\n",
    "        covar_full = self.covar_module(x)\n",
    "\n",
    "        mean_x = self.pooler_1d(mean_full)\n",
    "        covar_x = self.pooler_2d(covar_full)\n",
    "        \n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize likelihood and model\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "# model = ExactGPModelEnergies(train_X, train_Y, likelihood)\n",
    "model = ExactGPModelEnergiesMulti(train_X, train_Y, likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected 2 to 3 dimensions, but got 1-dimensional tensor for argument #1 'self' (while checking arguments for avg_pool1d)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/dlbox2/repos/rotor-gp/code/fit_energies.ipynb セル 21\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/dlbox2/repos/rotor-gp/code/fit_energies.ipynb#Y100sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m cov \u001b[39m=\u001b[39m model(train_X)\u001b[39m.\u001b[39mcovariance_matrix\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/dlbox2/repos/rotor-gp/code/fit_energies.ipynb#Y100sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(cov\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/anaconda3/envs/fande/lib/python3.10/site-packages/gpytorch/models/exact_gp.py:257\u001b[0m, in \u001b[0;36mExactGP.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mall\u001b[39m(torch\u001b[39m.\u001b[39mequal(train_input, \u001b[39minput\u001b[39m) \u001b[39mfor\u001b[39;00m train_input, \u001b[39minput\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(train_inputs, inputs)):\n\u001b[1;32m    256\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mYou must train on the training inputs!\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 257\u001b[0m     res \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(\u001b[39m*\u001b[39;49minputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    258\u001b[0m     \u001b[39mreturn\u001b[39;00m res\n\u001b[1;32m    260\u001b[0m \u001b[39m# Prior mode\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/fande/lib/python3.10/site-packages/gpytorch/module.py:30\u001b[0m, in \u001b[0;36mModule.__call__\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39minputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 30\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49minputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     31\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(outputs, \u001b[39mlist\u001b[39m):\n\u001b[1;32m     32\u001b[0m         \u001b[39mreturn\u001b[39;00m [_validate_module_outputs(output) \u001b[39mfor\u001b[39;00m output \u001b[39min\u001b[39;00m outputs]\n",
      "\u001b[1;32m/home/dlbox2/repos/rotor-gp/code/fit_energies.ipynb セル 21\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dlbox2/repos/rotor-gp/code/fit_energies.ipynb#Y100sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m mean_full \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmean_module(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dlbox2/repos/rotor-gp/code/fit_energies.ipynb#Y100sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m covar_full \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcovar_module(x)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/dlbox2/repos/rotor-gp/code/fit_energies.ipynb#Y100sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m mean_x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpooler_1d(mean_full)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dlbox2/repos/rotor-gp/code/fit_energies.ipynb#Y100sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m covar_x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler_2d(covar_full)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dlbox2/repos/rotor-gp/code/fit_energies.ipynb#Y100sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m \u001b[39mreturn\u001b[39;00m gpytorch\u001b[39m.\u001b[39mdistributions\u001b[39m.\u001b[39mMultivariateNormal(mean_x, covar_x)\n",
      "File \u001b[0;32m~/anaconda3/envs/fande/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/fande/lib/python3.10/site-packages/torch/nn/modules/pooling.py:559\u001b[0m, in \u001b[0;36mAvgPool1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 559\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mavg_pool1d(\n\u001b[1;32m    560\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkernel_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mceil_mode,\n\u001b[1;32m    561\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcount_include_pad)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 2 to 3 dimensions, but got 1-dimensional tensor for argument #1 'self' (while checking arguments for avg_pool1d)"
     ]
    }
   ],
   "source": [
    "cov = model(train_X).covariance_matrix\n",
    "print(cov.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = torch.zeros((40,40))\n",
    "\n",
    "i = 0\n",
    "j = 1\n",
    "covar_ij = model.covar_module(train_X[i,:,:], train_X[j,:,:]).evaluate().sum()\n",
    "\n",
    "for i in range(40):\n",
    "    for j in range(40):\n",
    "        covar_ij = model.covar_module(train_X[i,:,:], train_X[j,:,:]).evaluate().sum()\n",
    "        C[i,j] = covar_ij\n",
    "        # print(f\"{i} {j} {covar_ij}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(40):\n",
    "        print(C[0,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is for running the notebook in our testing framework\n",
    "import os\n",
    "\n",
    "\n",
    "# initialize likelihood and model\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "# model = ExactGPModelEnergies(train_X, train_Y, likelihood)\n",
    "model = ExactGPModelEnergiesMulti(train_X, train_Y, likelihood)\n",
    "\n",
    "\n",
    "training_iter = 10\n",
    "\n",
    "\n",
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "print(\"Training...\")\n",
    "# initial_loss = -mll(model(train_X), train_Y).item()\n",
    "for i in range(training_iter):\n",
    "    # Zero gradients from previous iteration\n",
    "    optimizer.zero_grad()\n",
    "    print(\"grad zeroed\")\n",
    "    # Output from model\n",
    "    output = model(train_X)\n",
    "    print(\"Output calculated\")\n",
    "    print(output)\n",
    "    # Calc loss and backprop gradients\n",
    "    loss = -mll(output, train_Y)\n",
    "    print(\"Loss calculated\")\n",
    "\n",
    "    loss.backward()\n",
    "    # print(loss)\n",
    "    # print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f   noise: %.3f' % (\n",
    "    #     i + 1, training_iter, loss.item(),\n",
    "    #     model.covar_module.base_kernel.lengthscale.item(),\n",
    "    #     model.likelihood.noise.item()\n",
    "    # ))\n",
    "    print(loss)\n",
    "    optimizer.step()\n",
    "\n",
    "print(\"Initial loss: \", initial_loss)\n",
    "print(\"Final loss: \", loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval\n",
    "\n",
    "pred = likelihood(model.forward(test_X)).mean\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get into evaluation (predictive posterior) mode\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "test_x = test_X\n",
    "\n",
    "# Test points are regularly spaced along [0,1]\n",
    "# Make predictions by feeding model through likelihood\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "#     test_x = torch.linspace(0, 1, 51)\n",
    "    observed_pred = likelihood(model.forward(test_x))\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(observed_pred.mean)\n",
    "plt.plot(test_Y)\n",
    "plt.xlabel('step')\n",
    "plt.ylabel('Predicted Energy')\n",
    "plt.show()\n",
    "\n",
    "# error = observed_pred.mean() - np.array(test_Y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
