{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dCOI-tn1woN"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tsitsvero/rotor-gp/blob/main/pack/train_eval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGkUAB7q1woR"
      },
      "source": [
        "# Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "znyGV5dF1woR",
        "outputId": "7e363cf9-78c6-44ea-dbe4-0621d45f020c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/lab-cosmo/librascal\n",
            "  Cloning https://github.com/lab-cosmo/librascal to /tmp/pip-req-build-j0sho39x\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/lab-cosmo/librascal /tmp/pip-req-build-j0sho39x\n",
            "  Resolved https://github.com/lab-cosmo/librascal to commit 6c55e99720f9a181f1efc5c7cd4976c87809d79b\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting git+https://github.com/chem-gp/fande\n",
            "  Cloning https://github.com/chem-gp/fande to /tmp/pip-req-build-2rd3t97a\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/chem-gp/fande /tmp/pip-req-build-2rd3t97a\n",
            "  Resolved https://github.com/chem-gp/fande to commit aded96b5f6a1eec9b699457a47248bcb58f290d1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gpytorch\n",
            "  Downloading gpytorch-1.11-py3-none-any.whl (266 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.1/266.1 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytorch-lightning\n",
            "  Downloading pytorch_lightning-2.1.3-py3-none-any.whl (777 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m777.7/777.7 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wandb\n",
            "  Downloading wandb-0.16.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xtb\n",
            "  Downloading xtb-22.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (17.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nbstripout\n",
            "  Downloading nbstripout-0.6.1-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from rascal==0.0.0) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from rascal==0.0.0) (1.11.4)\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from rascal==0.0.0) (3.7.1)\n",
            "Collecting ase>=3.19.0 (from rascal==0.0.0)\n",
            "  Downloading ase-3.22.1-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from gpytorch) (1.2.2)\n",
            "Collecting linear-operator>=0.5.0 (from gpytorch)\n",
            "  Downloading linear_operator-0.5.2-py3-none-any.whl (175 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.6/175.6 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2.1.0+cu121)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.66.1)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (6.0.1)\n",
            "Requirement already satisfied: fsspec[http]>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2023.6.0)\n",
            "Collecting torchmetrics>=0.7.0 (from pytorch-lightning)\n",
            "  Downloading torchmetrics-1.2.1-py3-none-any.whl (806 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m806.1/806.1 kB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (23.2)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.5.0)\n",
            "Collecting lightning-utilities>=0.8.0 (from pytorch-lightning)\n",
            "  Downloading lightning_utilities-0.10.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.39.1-py2.py3-none-any.whl (254 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.1/254.1 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from xtb) (1.16.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from nbstripout) (5.9.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.9.1)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jaxtyping>=0.2.9 (from linear-operator>=0.5.0->gpytorch)\n",
            "  Downloading jaxtyping-0.2.25-py3-none-any.whl (39 kB)\n",
            "Collecting typeguard~=2.13.3 (from linear-operator>=0.5.0->gpytorch)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->rascal==0.0.0) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->rascal==0.0.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->rascal==0.0.0) (4.46.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->rascal==0.0.0) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->rascal==0.0.0) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->rascal==0.0.0) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->rascal==0.0.0) (2.8.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2023.11.17)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch-lightning) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch-lightning) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch-lightning) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch-lightning) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch-lightning) (2.1.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->xtb) (2.21)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat->nbstripout) (2.19.0)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat->nbstripout) (4.19.2)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.10/dist-packages (from nbformat->nbstripout) (5.5.1)\n",
            "Requirement already satisfied: traitlets>=5.1 in /usr/local/lib/python3.10/dist-packages (from nbformat->nbstripout) (5.7.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->gpytorch) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->gpytorch) (3.2.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (4.0.3)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->nbstripout) (2023.11.2)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->nbstripout) (0.32.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->nbstripout) (0.15.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.12.0->pytorch-lightning) (2.1.3)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core->nbformat->nbstripout) (4.1.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.12.0->pytorch-lightning) (1.3.0)\n",
            "Building wheels for collected packages: rascal, fande\n",
            "  Building wheel for rascal (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rascal: filename=rascal-0.0.0-cp310-cp310-linux_x86_64.whl size=1160756 sha256=1c431a7a16b99a84787560cf0522423d06f77b9df7f20761e6717d758aae9750\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-fe2d9aa7/wheels/bc/72/69/ef99799b87ca514fefd4d408f33fd8e56427852b1cff5fe55a\n",
            "  Building wheel for fande (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fande: filename=fande-0.0.0-py3-none-any.whl size=54564 sha256=91c345a892aec814ae798308e6a714ff130b29c5ab5674701388677082eaa88a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-fe2d9aa7/wheels/30/ba/84/411722251f2aee25486664e17c8b197bbc34350633c9288672\n",
            "Successfully built rascal fande\n",
            "Installing collected packages: typeguard, smmap, setproctitle, sentry-sdk, lightning-utilities, fande, docker-pycreds, xtb, jaxtyping, gitdb, torchmetrics, linear-operator, GitPython, ase, wandb, rascal, pytorch-lightning, gpytorch, nbstripout\n",
            "Successfully installed GitPython-3.1.40 ase-3.22.1 docker-pycreds-0.4.0 fande-0.0.0 gitdb-4.0.11 gpytorch-1.11 jaxtyping-0.2.25 lightning-utilities-0.10.0 linear-operator-0.5.2 nbstripout-0.6.1 pytorch-lightning-2.1.3 rascal-0.0.0 sentry-sdk-1.39.1 setproctitle-1.3.3 smmap-5.0.1 torchmetrics-1.2.1 typeguard-2.13.3 wandb-0.16.1 xtb-22.1\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/lab-cosmo/librascal gpytorch pytorch-lightning wandb git+https://github.com/chem-gp/fande xtb nbstripout\n",
        "! nbstripout --install --global"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Davwyq8M136x",
        "outputId": "bb54ac6f-9eee-4bd5-834f-221cca00af13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1vtf8fz1woT"
      },
      "source": [
        "## Cooking up the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "LRz1N5Qq1woU",
        "outputId": "0b912c83-6761-4b21-b090-cb518932f74e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DATA_DIR /content/drive/MyDrive/data/\n",
            "RESULTS_DIR /content/results\n",
            "ENERGY_MODEL variational_inducing_points\n",
            "ENERGY_NUM_INDUCING_POINTS 10\n",
            "ENERGY_LR 0.1\n",
            "ENERGY_NUM_STEPS 5\n",
            "FORCES_MODEL variational_inducing_points\n",
            "FORCES_NUM_INDUCING_POINTS 10\n",
            "NUM_FORCE_SAMPLES 10\n",
            "FORCES_LR 0.1\n",
            "FORCES_NUM_STEPS 5\n",
            "PREDICTOR_NAME test.pth\n",
            "5 5\n",
            "fande.models module imported...\n",
            "Icecream logger is not available\n",
            "Total number of found groups: 14\n",
            "Checking if all atoms are covered:  True\n",
            "Total length of traj is 5\n",
            "Total number of batches 5\n",
            "Calculating invariants on trajectory with librascal...\n",
            "100% 5/5 [00:00<00:00, 17.45it/s]\n",
            "invariants for energy fitting calculated\n",
            "100% 5/5 [00:10<00:00,  2.19s/it]\n",
            "invariants for forces fitting calculated\n",
            "torch.Size([5, 1200])\n",
            "torch.Size([5])\n",
            "Dataloader for group 0 created\n",
            "Number of samples in dataloader: 10\n",
            "Dataloader for group 1 created\n",
            "Number of samples in dataloader: 10\n",
            "Dataloader for group 2 created\n",
            "Number of samples in dataloader: 10\n",
            "Dataloader for group 3 created\n",
            "Number of samples in dataloader: 10\n",
            "Dataloader for group 4 created\n",
            "Number of samples in dataloader: 10\n",
            "Dataloader for group 5 created\n",
            "Number of samples in dataloader: 10\n",
            "Dataloader for group 6 created\n",
            "Number of samples in dataloader: 10\n",
            "Dataloader for group 7 created\n",
            "Number of samples in dataloader: 10\n",
            "Dataloader for group 8 created\n",
            "Number of samples in dataloader: 10\n",
            "Dataloader for group 9 created\n",
            "Number of samples in dataloader: 10\n",
            "Dataloader for group 10 created\n",
            "Number of samples in dataloader: 10\n",
            "Dataloader for group 11 created\n",
            "Number of samples in dataloader: 10\n",
            "Dataloader for group 12 created\n",
            "Number of samples in dataloader: 10\n",
            "Dataloader for group 13 created\n",
            "Number of samples in dataloader: 10\n",
            "Training with inducing points:  torch.Size([5, 1200])\n",
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "Training energy model\n",
            "Missing logger folder: /content/lightning_logs\n",
            "2023-12-30 13:22:58.957634: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-30 13:22:58.957724: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-30 13:22:58.960650: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-30 13:23:00.560130: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\n",
            "  | Name       | Type                    | Params\n",
            "-------------------------------------------------------\n",
            "0 | likelihood | GaussianLikelihood      | 1     \n",
            "1 | model      | SVGPModel               | 7.2 K \n",
            "2 | mll        | PredictiveLogLikelihood | 7.2 K \n",
            "-------------------------------------------------------\n",
            "7.2 K     Trainable params\n",
            "0         Non-trainable params\n",
            "7.2 K     Total params\n",
            "0.029     Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=1` in the `DataLoader` to improve performance.\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py:293: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=1000). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "Epoch 4: 100% 1/1 [00:00<00:00, 76.31it/s, v_num=0, loss=1.110]`Trainer.fit` stopped: `max_epochs=5` reached.\n",
            "Epoch 4: 100% 1/1 [00:00<00:00, 55.75it/s, v_num=0, loss=1.110]\n",
            "Energy model fitted\n",
            "Training with inducing points:  torch.Size([10, 1200])\n",
            "ModelForces initialized\n",
            "Training with inducing points:  torch.Size([10, 1200])\n",
            "ModelForces initialized\n",
            "Training with inducing points:  torch.Size([10, 1200])\n",
            "ModelForces initialized\n",
            "Training with inducing points:  torch.Size([10, 1200])\n",
            "ModelForces initialized\n",
            "Training with inducing points:  torch.Size([10, 1200])\n",
            "ModelForces initialized\n",
            "Training with inducing points:  torch.Size([10, 1200])\n",
            "ModelForces initialized\n",
            "Training with inducing points:  torch.Size([10, 1200])\n",
            "ModelForces initialized\n",
            "Training with inducing points:  torch.Size([10, 1200])\n",
            "ModelForces initialized\n",
            "Training with inducing points:  torch.Size([10, 1200])\n",
            "ModelForces initialized\n",
            "Training with inducing points:  torch.Size([10, 1200])\n",
            "ModelForces initialized\n",
            "Training with inducing points:  torch.Size([10, 1200])\n",
            "ModelForces initialized\n",
            "Training with inducing points:  torch.Size([10, 1200])\n",
            "ModelForces initialized\n",
            "Training with inducing points:  torch.Size([10, 1200])\n",
            "ModelForces initialized\n",
            "Training with inducing points:  torch.Size([10, 1200])\n",
            "ModelForces initialized\n",
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "Training force model 0 (Total 14 models)\n",
            "\n",
            "  | Name       | Type                    | Params\n",
            "-------------------------------------------------------\n",
            "0 | likelihood | GaussianLikelihood      | 1     \n",
            "1 | model      | SVGPModel               | 13.3 K\n",
            "2 | mll        | PredictiveLogLikelihood | 13.3 K\n",
            "-------------------------------------------------------\n",
            "13.3 K    Trainable params\n",
            "0         Non-trainable params\n",
            "13.3 K    Total params\n",
            "0.053     Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py:293: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "Epoch 4: 100% 1/1 [00:00<00:00, 73.17it/s, v_num=1, loss=2.860]`Trainer.fit` stopped: `max_epochs=5` reached.\n",
            "Epoch 4: 100% 1/1 [00:00<00:00, 52.74it/s, v_num=1, loss=2.860]\n",
            "Training force model 1 (Total 14 models)\n",
            "\n",
            "  | Name       | Type                    | Params\n",
            "-------------------------------------------------------\n",
            "0 | likelihood | GaussianLikelihood      | 1     \n",
            "1 | model      | SVGPModel               | 13.3 K\n",
            "2 | mll        | PredictiveLogLikelihood | 13.3 K\n",
            "-------------------------------------------------------\n",
            "13.3 K    Trainable params\n",
            "0         Non-trainable params\n",
            "13.3 K    Total params\n",
            "0.053     Total estimated model params size (MB)\n",
            "Epoch 4: 100% 1/1 [00:00<00:00, 74.74it/s, v_num=2, loss=1.660]`Trainer.fit` stopped: `max_epochs=5` reached.\n",
            "Epoch 4: 100% 1/1 [00:00<00:00, 53.63it/s, v_num=2, loss=1.660]\n",
            "Training force model 2 (Total 14 models)\n",
            "\n",
            "  | Name       | Type                    | Params\n",
            "-------------------------------------------------------\n",
            "0 | likelihood | GaussianLikelihood      | 1     \n",
            "1 | model      | SVGPModel               | 13.3 K\n",
            "2 | mll        | PredictiveLogLikelihood | 13.3 K\n",
            "-------------------------------------------------------\n",
            "13.3 K    Trainable params\n",
            "0         Non-trainable params\n",
            "13.3 K    Total params\n",
            "0.053     Total estimated model params size (MB)\n",
            "Epoch 4: 100% 1/1 [00:00<00:00, 75.33it/s, v_num=3, loss=1.980]`Trainer.fit` stopped: `max_epochs=5` reached.\n",
            "Epoch 4: 100% 1/1 [00:00<00:00, 54.31it/s, v_num=3, loss=1.980]\n",
            "Training force model 3 (Total 14 models)\n",
            "\n",
            "  | Name       | Type                    | Params\n",
            "-------------------------------------------------------\n",
            "0 | likelihood | GaussianLikelihood      | 1     \n",
            "1 | model      | SVGPModel               | 13.3 K\n",
            "2 | mll        | PredictiveLogLikelihood | 13.3 K\n",
            "-------------------------------------------------------\n",
            "13.3 K    Trainable params\n",
            "0         Non-trainable params\n",
            "13.3 K    Total params\n",
            "0.053     Total estimated model params size (MB)\n",
            "Epoch 4: 100% 1/1 [00:00<00:00, 80.53it/s, v_num=4, loss=1.580]`Trainer.fit` stopped: `max_epochs=5` reached.\n",
            "Epoch 4: 100% 1/1 [00:00<00:00, 57.98it/s, v_num=4, loss=1.580]\n",
            "Training force model 4 (Total 14 models)\n",
            "\n",
            "  | Name       | Type                    | Params\n",
            "-------------------------------------------------------\n",
            "0 | likelihood | GaussianLikelihood      | 1     \n",
            "1 | model      | SVGPModel               | 13.3 K\n",
            "2 | mll        | PredictiveLogLikelihood | 13.3 K\n",
            "-------------------------------------------------------\n",
            "13.3 K    Trainable params\n",
            "0         Non-trainable params\n",
            "13.3 K    Total params\n",
            "0.053     Total estimated model params size (MB)\n",
            "Epoch 4: 100% 1/1 [00:00<00:00, 73.60it/s, v_num=5, loss=3.320]`Trainer.fit` stopped: `max_epochs=5` reached.\n",
            "Epoch 4: 100% 1/1 [00:00<00:00, 52.31it/s, v_num=5, loss=3.320]\n",
            "Training force model 5 (Total 14 models)\n",
            "\n",
            "  | Name       | Type                    | Params\n",
            "-------------------------------------------------------\n",
            "0 | likelihood | GaussianLikelihood      | 1     \n",
            "1 | model      | SVGPModel               | 13.3 K\n",
            "2 | mll        | PredictiveLogLikelihood | 13.3 K\n",
            "-------------------------------------------------------\n",
            "13.3 K    Trainable params\n",
            "0         Non-trainable params\n",
            "13.3 K    Total params\n",
            "0.053     Total estimated model params size (MB)\n",
            "Epoch 4: 100% 1/1 [00:00<00:00, 57.46it/s, v_num=6, loss=1.890]`Trainer.fit` stopped: `max_epochs=5` reached.\n",
            "Epoch 4: 100% 1/1 [00:00<00:00, 43.63it/s, v_num=6, loss=1.890]\n",
            "Training force model 6 (Total 14 models)\n",
            "\n",
            "  | Name       | Type                    | Params\n",
            "-------------------------------------------------------\n",
            "0 | likelihood | GaussianLikelihood      | 1     \n",
            "1 | model      | SVGPModel               | 13.3 K\n",
            "2 | mll        | PredictiveLogLikelihood | 13.3 K\n",
            "-------------------------------------------------------\n",
            "13.3 K    Trainable params\n",
            "0         Non-trainable params\n",
            "13.3 K    Total params\n",
            "0.053     Total estimated model params size (MB)\n",
            "Epoch 4: 100% 1/1 [00:00<00:00, 74.35it/s, v_num=7, loss=2.950]`Trainer.fit` stopped: `max_epochs=5` reached.\n",
            "Epoch 4: 100% 1/1 [00:00<00:00, 53.78it/s, v_num=7, loss=2.950]\n",
            "Training force model 7 (Total 14 models)\n",
            "\n",
            "  | Name       | Type                    | Params\n",
            "-------------------------------------------------------\n",
            "0 | likelihood | GaussianLikelihood      | 1     \n",
            "1 | model      | SVGPModel               | 13.3 K\n",
            "2 | mll        | PredictiveLogLikelihood | 13.3 K\n",
            "-------------------------------------------------------\n",
            "13.3 K    Trainable params\n",
            "0         Non-trainable params\n",
            "13.3 K    Total params\n",
            "0.053     Total estimated model params size (MB)\n",
            "Epoch 4: 100% 1/1 [00:00<00:00, 64.84it/s, v_num=8, loss=1.230]`Trainer.fit` stopped: `max_epochs=5` reached.\n",
            "Epoch 4: 100% 1/1 [00:00<00:00, 43.02it/s, v_num=8, loss=1.230]\n",
            "Training force model 8 (Total 14 models)\n",
            "\n",
            "  | Name       | Type                    | Params\n",
            "-------------------------------------------------------\n",
            "0 | likelihood | GaussianLikelihood      | 1     \n",
            "1 | model      | SVGPModel               | 13.3 K\n",
            "2 | mll        | PredictiveLogLikelihood | 13.3 K\n",
            "-------------------------------------------------------\n",
            "13.3 K    Trainable params\n",
            "0         Non-trainable params\n",
            "13.3 K    Total params\n",
            "0.053     Total estimated model params size (MB)\n",
            "Epoch 4: 100% 1/1 [00:00<00:00, 66.33it/s, v_num=9, loss=1.520]`Trainer.fit` stopped: `max_epochs=5` reached.\n",
            "Epoch 4: 100% 1/1 [00:00<00:00, 47.77it/s, v_num=9, loss=1.520]\n",
            "Training force model 9 (Total 14 models)\n",
            "\n",
            "  | Name       | Type                    | Params\n",
            "-------------------------------------------------------\n",
            "0 | likelihood | GaussianLikelihood      | 1     \n",
            "1 | model      | SVGPModel               | 13.3 K\n",
            "2 | mll        | PredictiveLogLikelihood | 13.3 K\n",
            "-------------------------------------------------------\n",
            "13.3 K    Trainable params\n",
            "0         Non-trainable params\n",
            "13.3 K    Total params\n",
            "0.053     Total estimated model params size (MB)\n",
            "Epoch 4: 100% 1/1 [00:00<00:00, 78.92it/s, v_num=10, loss=1.210]`Trainer.fit` stopped: `max_epochs=5` reached.\n",
            "Epoch 4: 100% 1/1 [00:00<00:00, 56.77it/s, v_num=10, loss=1.210]\n",
            "Training force model 10 (Total 14 models)\n",
            "\n",
            "  | Name       | Type                    | Params\n",
            "-------------------------------------------------------\n",
            "0 | likelihood | GaussianLikelihood      | 1     \n",
            "1 | model      | SVGPModel               | 13.3 K\n",
            "2 | mll        | PredictiveLogLikelihood | 13.3 K\n",
            "-------------------------------------------------------\n",
            "13.3 K    Trainable params\n",
            "0         Non-trainable params\n",
            "13.3 K    Total params\n",
            "0.053     Total estimated model params size (MB)\n",
            "Epoch 4: 100% 1/1 [00:00<00:00, 70.73it/s, v_num=11, loss=1.660]`Trainer.fit` stopped: `max_epochs=5` reached.\n",
            "Epoch 4: 100% 1/1 [00:00<00:00, 52.08it/s, v_num=11, loss=1.660]\n",
            "Training force model 11 (Total 14 models)\n",
            "\n",
            "  | Name       | Type                    | Params\n",
            "-------------------------------------------------------\n",
            "0 | likelihood | GaussianLikelihood      | 1     \n",
            "1 | model      | SVGPModel               | 13.3 K\n",
            "2 | mll        | PredictiveLogLikelihood | 13.3 K\n",
            "-------------------------------------------------------\n",
            "13.3 K    Trainable params\n",
            "0         Non-trainable params\n",
            "13.3 K    Total params\n",
            "0.053     Total estimated model params size (MB)\n",
            "Epoch 4: 100% 1/1 [00:00<00:00, 78.93it/s, v_num=12, loss=2.040]`Trainer.fit` stopped: `max_epochs=5` reached.\n",
            "Epoch 4: 100% 1/1 [00:00<00:00, 56.56it/s, v_num=12, loss=2.040]\n",
            "Training force model 12 (Total 14 models)\n",
            "\n",
            "  | Name       | Type                    | Params\n",
            "-------------------------------------------------------\n",
            "0 | likelihood | GaussianLikelihood      | 1     \n",
            "1 | model      | SVGPModel               | 13.3 K\n",
            "2 | mll        | PredictiveLogLikelihood | 13.3 K\n",
            "-------------------------------------------------------\n",
            "13.3 K    Trainable params\n",
            "0         Non-trainable params\n",
            "13.3 K    Total params\n",
            "0.053     Total estimated model params size (MB)\n",
            "Epoch 4: 100% 1/1 [00:00<00:00, 61.98it/s, v_num=13, loss=2.330]`Trainer.fit` stopped: `max_epochs=5` reached.\n",
            "Epoch 4: 100% 1/1 [00:00<00:00, 45.88it/s, v_num=13, loss=2.330]\n",
            "Training force model 13 (Total 14 models)\n",
            "\n",
            "  | Name       | Type                    | Params\n",
            "-------------------------------------------------------\n",
            "0 | likelihood | GaussianLikelihood      | 1     \n",
            "1 | model      | SVGPModel               | 13.3 K\n",
            "2 | mll        | PredictiveLogLikelihood | 13.3 K\n",
            "-------------------------------------------------------\n",
            "13.3 K    Trainable params\n",
            "0         Non-trainable params\n",
            "13.3 K    Total params\n",
            "0.053     Total estimated model params size (MB)\n",
            "Epoch 4: 100% 1/1 [00:00<00:00, 77.72it/s, v_num=14, loss=3.200]`Trainer.fit` stopped: `max_epochs=5` reached.\n",
            "Epoch 4: 100% 1/1 [00:00<00:00, 55.96it/s, v_num=14, loss=3.200]\n",
            "Saving predictor requires humongous amount of memory! Spare some dozens of GBs!\n",
            "  0% 0/5 [00:00<?, ?it/s]Time for invariants (call from forces):  2042.2918796539307\n",
            "Predicting forces...\n",
            "Energy model summary: \n",
            "Time invariants:  0.15878677368164062\n",
            "Time prediction:  13.992786407470703\n",
            "Time moving on device:  0.8192062377929688\n",
            "Time total:  16.375303268432617\n",
            "[[ 0.14704984  0.15179273  0.14838652]\n",
            " [ 0.11401328  0.10324623  0.08450374]\n",
            " [ 0.14529115  0.11221214  0.10040572]\n",
            " ...\n",
            " [ 0.0391053  -0.07271059  0.14124992]\n",
            " [-0.07530954  0.1144464   0.14865759]\n",
            " [ 0.1476434   0.04588533 -0.05961114]]\n",
            "[[5.26658249 5.33519745 5.20646334]\n",
            " [4.87718248 5.03625011 4.87740517]\n",
            " [5.10021925 4.7831831  5.18953323]\n",
            " ...\n",
            " [1.86061907 1.90061235 1.8325671 ]\n",
            " [1.9009881  1.83742595 1.82674837]\n",
            " [1.83147538 1.85789394 1.89766216]]\n",
            "[-3101.304]\n",
            " 20% 1/5 [00:02<00:08,  2.18s/it]Time for invariants (call from forces):  2712.6264572143555\n",
            "Predicting forces...\n",
            "Energy model summary: \n",
            "Time invariants:  0.1575946807861328\n",
            "Time prediction:  10.182619094848633\n",
            "Time moving on device:  5.746364593505859\n",
            "Time total:  18.135786056518555\n",
            "[[ 0.14678445  0.15170152  0.14854737]\n",
            " [ 0.11421837  0.09984662  0.08447023]\n",
            " [ 0.14499265  0.11193751  0.10025743]\n",
            " ...\n",
            " [ 0.04129676 -0.07351622  0.1424337 ]\n",
            " [-0.07327121  0.11538305  0.15379062]\n",
            " [ 0.14792909  0.04600653 -0.05903699]]\n",
            "[[5.26250076 5.33447266 5.2021513 ]\n",
            " [4.91230249 5.12347126 4.87905788]\n",
            " [5.10555363 4.74100971 5.19040585]\n",
            " ...\n",
            " [1.86007643 1.90114856 1.83209276]\n",
            " [1.90064108 1.83712602 1.8248601 ]\n",
            " [1.8313427  1.85784519 1.89757824]]\n",
            "[-3101.3062]\n",
            " 40% 2/5 [00:05<00:07,  2.59s/it]Time for invariants (call from forces):  3021.05450630188\n",
            "Predicting forces...\n",
            "Energy model summary: \n",
            "Time invariants:  0.18668174743652344\n",
            "Time prediction:  5.9947967529296875\n",
            "Time moving on device:  1.0704994201660156\n",
            "Time total:  9.285688400268555\n",
            "[[ 0.14644071  0.15163366  0.14871809]\n",
            " [ 0.1144027   0.09515984  0.09152184]\n",
            " [ 0.14468831  0.11092981  0.09983829]\n",
            " ...\n",
            " [ 0.04396451 -0.0721105   0.14272687]\n",
            " [-0.07056281  0.11472653  0.15700147]\n",
            " [ 0.14829248  0.0461287  -0.05829197]]\n",
            "[[5.25882912 5.33287621 5.19812298]\n",
            " [4.95158768 5.0911684  4.94410992]\n",
            " [5.11398792 4.64215517 5.19219732]\n",
            " ...\n",
            " [1.85935092 1.90028048 1.83201933]\n",
            " [1.89970422 1.83757734 1.82389665]\n",
            " [1.83118391 1.85779798 1.8974514 ]]\n",
            "[-3101.3054]\n",
            " 60% 3/5 [00:08<00:05,  2.84s/it]Time for invariants (call from forces):  2327.1753787994385\n",
            "Predicting forces...\n",
            "Energy model summary: \n",
            "Time invariants:  0.3638267517089844\n",
            "Time prediction:  15.892982482910156\n",
            "Time moving on device:  1.718282699584961\n",
            "Time total:  20.08509635925293\n",
            "[[ 0.14602874  0.15156037  0.14892146]\n",
            " [ 0.11422652  0.09407806  0.09426171]\n",
            " [ 0.14442962  0.10914402  0.09941594]\n",
            " ...\n",
            " [ 0.04653785 -0.06837163  0.14174521]\n",
            " [-0.06965497  0.11223769  0.15633854]\n",
            " [ 0.14866948  0.04632063 -0.05756512]]\n",
            "[[5.25546551 5.33060932 5.19417334]\n",
            " [4.94938183 5.07629347 4.93377161]\n",
            " [5.12257671 4.51342154 5.19371986]\n",
            " ...\n",
            " [1.85860944 1.89777112 1.83242905]\n",
            " [1.89929032 1.83874965 1.82434964]\n",
            " [1.83102751 1.85772729 1.89731956]]\n",
            "[-3101.3027]\n",
            " 80% 4/5 [00:10<00:02,  2.72s/it]Time for invariants (call from forces):  2698.918342590332\n",
            "Predicting forces...\n",
            "Energy model summary: \n",
            "Time invariants:  0.3418922424316406\n",
            "Time prediction:  7.013797760009766\n",
            "Time moving on device:  1.4958381652832031\n",
            "Time total:  10.508298873901367\n",
            "[[ 0.14555384  0.15146555  0.14915945]\n",
            " [ 0.11406223  0.09444068  0.09573643]\n",
            " [ 0.14425404  0.10711047  0.09926948]\n",
            " ...\n",
            " [ 0.04803771 -0.06407538  0.14001215]\n",
            " [-0.06878839  0.10897813  0.15515837]\n",
            " [ 0.14898929  0.04636767 -0.05687721]]\n",
            "[[5.25276947 5.32800484 5.19074345]\n",
            " [4.95718575 5.07992554 4.92444611]\n",
            " [5.12998581 4.40223598 5.19493294]\n",
            " ...\n",
            " [1.8581903  1.89469385 1.8331455 ]\n",
            " [1.89885402 1.84017253 1.82500398]\n",
            " [1.83091426 1.85770679 1.89715743]]\n",
            "[-3101.2993]\n",
            "100% 5/5 [00:13<00:00,  2.71s/it]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "# DATA_DIR = os.path.expanduser(\"~/repos/data/\")\n",
        "# # DATA_DIR = \"/data1/simulations/datasets/rotors/high_temp_ML_training_data/\"\n",
        "# RESULTS_DIR = os.path.expanduser(\"~/repos/data/results\")\n",
        "\n",
        "\n",
        "DATA_DIR = os.path.expanduser(\"/content/drive/MyDrive/data/\")\n",
        "# FANDE_DIR = os.path.expanduser(\"~/\")\n",
        "RESULTS_DIR = os.path.expanduser(\"/content/results\")\n",
        "# os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "ENERGY_MODEL = 'variational_inducing_points' #'variational_inducing_points', 'exact'\n",
        "ENERGY_NUM_INDUCING_POINTS = 10\n",
        "ENERGY_LR = 0.1\n",
        "ENERGY_NUM_STEPS = 5\n",
        "\n",
        "FORCES_MODEL = 'variational_inducing_points' #'variational_inducing_points', 'exact'\n",
        "FORCES_NUM_INDUCING_POINTS = 10\n",
        "NUM_FORCE_SAMPLES = 10\n",
        "FORCES_LR = 0.1\n",
        "FORCES_NUM_STEPS = 5\n",
        "\n",
        "PREDICTOR_NAME = 'test.pth'\n",
        "SUBSAMPLE = 200 # subsample data to reduce time durings tests\n",
        "\n",
        "! python cook_model.py \\\n",
        "--data_dir $DATA_DIR \\\n",
        "--results_dir $RESULTS_DIR \\\n",
        "--energy_model $ENERGY_MODEL \\\n",
        "--energy_num_inducing_points $ENERGY_NUM_INDUCING_POINTS \\\n",
        "--energy_lr $ENERGY_LR \\\n",
        "--energy_num_steps $ENERGY_NUM_STEPS \\\n",
        "--forces_model $FORCES_MODEL \\\n",
        "--forces_num_inducing_points $FORCES_NUM_INDUCING_POINTS \\\n",
        "--num_force_samples $NUM_FORCE_SAMPLES \\\n",
        "--forces_lr $FORCES_LR \\\n",
        "--forces_num_steps $FORCES_NUM_STEPS \\\n",
        "--predictor_name $PREDICTOR_NAME \\\n",
        "--subsample $SUBSAMPLE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ry9M9pUc1woU"
      },
      "source": [
        "## Testing models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Q-igKC1a1woV"
      },
      "outputs": [],
      "source": [
        "from ase import io\n",
        "test_traj = io.read(DATA_DIR + \"/results_triasine_ML_2000/struct_295_295K/md_trajectory.traj\", index=\"1000:1010\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "81lptjK31woV",
        "outputId": "86208dd5-f9f8-4e6c-9036-1fd3b6171dc2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fande:Fande module imported\n",
            "WARNING:rascal.utils.filter:Warning: skmatter module not found. CUR and FPS filters will be unavailable.\n",
            "WARNING:rascal.utils.filter:Original error:\n",
            "No module named 'skmatter'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fande.models module imported...\n",
            "Icecream logger is not available\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import torch\n",
        "FANDE_DIR = os.path.expanduser(\"~/repos/\")\n",
        "sys.path.append(FANDE_DIR + \"fande/\")\n",
        "\n",
        "from fande.predict import FandePredictor\n",
        "from fande.ase import FandeCalc\n",
        "# load the predictor:\n",
        "# predictor_loaded = torch.load(RESULTS_DIR + \"/fande_predictor.pth\")\n",
        "predictor_loaded = torch.load(RESULTS_DIR + \"/test.pth\")\n",
        "fande_calc_loaded = FandeCalc(predictor_loaded)\n",
        "device = torch.device('cpu')\n",
        "fande_calc_loaded.predictor.move_models_to_device(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "nc-Rh9361woX",
        "outputId": "4d36ffea-3698-459d-b6d7-6ff74f7e8389",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fande:Setting context for descriptors calculation to production\n",
            "INFO:fande:Setting context for descriptors calculation to production\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time for invariants (call from forces):  1783.3480834960938\n",
            "Predicting forces...\n",
            "Energy model summary: \n",
            "Time invariants:  0.15735626220703125\n",
            "Time prediction:  4.910707473754883\n",
            "Time moving on device:  0.7863044738769531\n",
            "Time total:  7.155179977416992\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fande:Setting context for descriptors calculation to production\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time for invariants (call from forces):  1619.107723236084\n",
            "Predicting forces...\n",
            "Energy model summary: \n",
            "Time invariants:  0.15354156494140625\n",
            "Time prediction:  3.579854965209961\n",
            "Time moving on device:  0.7348060607910156\n",
            "Time total:  5.687952041625977\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fande:Setting context for descriptors calculation to production\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time for invariants (call from forces):  1566.718339920044\n",
            "Predicting forces...\n",
            "Energy model summary: \n",
            "Time invariants:  0.1595020294189453\n",
            "Time prediction:  3.7271976470947266\n",
            "Time moving on device:  0.7991790771484375\n",
            "Time total:  5.922794342041016\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fande:Setting context for descriptors calculation to production\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time for invariants (call from forces):  2386.4898681640625\n",
            "Predicting forces...\n",
            "Energy model summary: \n",
            "Time invariants:  0.17905235290527344\n",
            "Time prediction:  7.97724723815918\n",
            "Time moving on device:  0.7779598236083984\n",
            "Time total:  10.164022445678711\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fande:Setting context for descriptors calculation to production\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time for invariants (call from forces):  2460.8988761901855\n",
            "Predicting forces...\n",
            "Energy model summary: \n",
            "Time invariants:  0.15497207641601562\n",
            "Time prediction:  4.335165023803711\n",
            "Time moving on device:  0.9164810180664062\n",
            "Time total:  6.691694259643555\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fande:Setting context for descriptors calculation to production\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time for invariants (call from forces):  1785.7556343078613\n",
            "Predicting forces...\n",
            "Energy model summary: \n",
            "Time invariants:  0.1766681671142578\n",
            "Time prediction:  4.158973693847656\n",
            "Time moving on device:  0.9658336639404297\n",
            "Time total:  6.671905517578125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fande:Setting context for descriptors calculation to production\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time for invariants (call from forces):  1575.7570266723633\n",
            "Predicting forces...\n",
            "Energy model summary: \n",
            "Time invariants:  0.15735626220703125\n",
            "Time prediction:  4.400014877319336\n",
            "Time moving on device:  1.001119613647461\n",
            "Time total:  7.058620452880859\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fande:Setting context for descriptors calculation to production\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time for invariants (call from forces):  1569.098711013794\n",
            "Predicting forces...\n",
            "Energy model summary: \n",
            "Time invariants:  0.16736984252929688\n",
            "Time prediction:  3.591299057006836\n",
            "Time moving on device:  0.7672309875488281\n",
            "Time total:  5.756139755249023\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fande:Setting context for descriptors calculation to production\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time for invariants (call from forces):  1571.5913772583008\n",
            "Predicting forces...\n",
            "Energy model summary: \n",
            "Time invariants:  0.15687942504882812\n",
            "Time prediction:  3.6416053771972656\n",
            "Time moving on device:  0.7643699645996094\n",
            "Time total:  5.811452865600586\n",
            "Time for invariants (call from forces):  1578.7808895111084\n",
            "Predicting forces...\n",
            "Energy model summary: \n",
            "Time invariants:  0.15091896057128906\n",
            "Time prediction:  3.762483596801758\n",
            "Time moving on device:  0.8096694946289062\n",
            "Time total:  6.184101104736328\n"
          ]
        }
      ],
      "source": [
        "forces_true = []\n",
        "forces_predicted = []\n",
        "\n",
        "energy_true = []\n",
        "energy_predicted = []\n",
        "\n",
        "for atoms in test_traj:\n",
        "    forces_true.append(atoms.get_forces())\n",
        "    energy_true.append(atoms.get_potential_energy())\n",
        "    atoms.set_calculator(fande_calc_loaded)\n",
        "    forces_predicted.append(atoms.get_forces())\n",
        "    energy_predicted.append(atoms.get_potential_energy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKCXpyfo1woX"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "energy_true = np.array(energy_true)\n",
        "energy_predicted = np.array(energy_predicted)\n",
        "energy_errors = energy_predicted - energy_true\n",
        "\n",
        "forces_true = np.array(forces_true)\n",
        "forces_predicted = np.array(forces_predicted)\n",
        "forces_errors = forces_predicted - forces_true\n",
        "\n",
        "atomic_groups = predictor_loaded.fdm.atomic_groups\n",
        "\n",
        "\n",
        "\n",
        "for ag in atomic_groups:\n",
        "\n",
        "    print(\"Atomic group\", ag)\n",
        "    print(\"F_x\")\n",
        "    print(\"MAE\", np.mean(np.abs(forces_errors[:, ag, 0].flatten())))\n",
        "    print(\"RMSE\", np.sqrt(np.mean(forces_errors[:, ag, 0].flatten()**2)))\n",
        "    print(\"F_y\")\n",
        "    print(\"MAE\", np.mean(np.abs(forces_errors[:, ag, 1].flatten())))\n",
        "    print(\"RMSE\", np.sqrt(np.mean(forces_errors[:, ag, 1].flatten()**2)))\n",
        "    print(\"F_z\")\n",
        "    print(\"MAE\", np.mean(np.abs(forces_errors[:, ag, 2].flatten())))\n",
        "    print(\"RMSE\", np.sqrt(np.mean(forces_errors[:, ag, 2].flatten()**2)))\n",
        "    print(\"E\")\n",
        "    print(\"MAE\", np.mean(np.abs(energy_errors.flatten())))\n",
        "    print(\"RMSE\", np.sqrt(np.mean(energy_errors.flatten()**2)))\n",
        "\n",
        "    fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(15, 3))\n",
        "    axs[0].set_title(r\"$F_x$\")\n",
        "    axs[0].plot(np.array(forces_true)[:, ag, 0].flatten(), label=\"true\")\n",
        "    axs[0].plot(np.array(forces_predicted)[:, ag, 0].flatten(), label=\"predicted\")\n",
        "    axs[0].legend()\n",
        "\n",
        "    axs[1].set_title(r\"$F_y$\")\n",
        "    axs[1].plot(np.array(forces_true)[:, ag, 1].flatten(), label=\"true\")\n",
        "    axs[1].plot(np.array(forces_predicted)[:, ag, 1].flatten(), label=\"predicted\")\n",
        "    axs[1].legend()\n",
        "\n",
        "    axs[2].set_title(r\"$F_z$\")\n",
        "    axs[2].plot(np.array(forces_true)[:, ag, 2].flatten(), label=\"true\")\n",
        "    axs[2].plot(np.array(forces_predicted)[:, ag, 2].flatten(), label=\"predicted\")\n",
        "    axs[2].legend()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(15, 3))\n",
        "\n",
        "    axs[0].set_title(r\"$F_x$ errors\")\n",
        "    axs[0].hist(forces_errors[:, ag, 0].flatten(), bins=100)\n",
        "\n",
        "    axs[1].set_title(r\"$F_y$ errors\")\n",
        "    axs[1].hist(forces_errors[:, ag, 1].flatten(), bins=100)\n",
        "\n",
        "    axs[2].set_title(r\"$F_z$ errors\")\n",
        "    axs[2].hist(forces_errors[:, ag, 2].flatten(), bins=100)\n",
        "    plt.show()\n",
        "\n",
        "    fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(15, 3))\n",
        "    axs[0].set_title(r\"$E$ errors\")\n",
        "    axs[0].hist(energy_errors.flatten(), bins=100)\n",
        "\n",
        "    axs[1].set_title(r\"$E$\")\n",
        "    axs[1].plot(energy_true, label=\"true\")\n",
        "    axs[1].plot(energy_predicted, label=\"predicted\")\n",
        "    axs[1].legend()\n",
        "    plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "test_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}