{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! jupyter nbconvert --to python --RegexRemovePreprocessor.patterns=\"^%\"  train_and_run.ipynb\n",
    "\n",
    "# RULES:\n",
    "# 1. Do not make plot.show() only plot.savefig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [03:19<00:00,  5.01it/s]\n",
      "100%|██████████| 20/20 [00:03<00:00,  5.18it/s]\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# import argparse\n",
    "\n",
    "# # Construct the argument parser\n",
    "# ap = argparse.ArgumentParser()\n",
    "\n",
    "# # Add the arguments to the parser\n",
    "# ap.add_argument(\"-a\", \"--foperand\", required=True,\n",
    "#    help=\"first operand\")\n",
    "# ap.add_argument(\"-b\", \"--soperand\", required=True,\n",
    "#    help=\"second operand\")\n",
    "# args = vars(ap.parse_args())\n",
    "\n",
    "# print(args)\n",
    "\n",
    "# print(\"Starting script...\")\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# export R=0; python ./train_and_run.py 2>&1 \n",
    "\n",
    "import ase\n",
    "from ase import io\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../../fande\") \n",
    "# sys.path.append(\"..\") \n",
    "import fande\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "os.environ['VASP_PP_PATH'] = \"/home/qklmn/repos/pseudos\"\n",
    "\n",
    "def make_calc_dir():\n",
    "    if os.getcwd()[-4:] != 'code':\n",
    "        os.chdir('../../../code')\n",
    "    dir_name = f'{datetime.now().strftime(\"%Y-%m-%d_%H %M-%S_%f\")}'\n",
    "    dir_name = '../results/train_and_run/' + dir_name\n",
    "    os.makedirs(dir_name, exist_ok=True)\n",
    "    abs_dir_path = os.path.abspath(dir_name)\n",
    "    return abs_dir_path\n",
    "\n",
    "temp_dir = make_calc_dir()\n",
    "os.chdir(temp_dir)\n",
    "print(\"Saving data to directory: \", temp_dir)\n",
    "\n",
    "\n",
    "import sys\n",
    "log_file = temp_dir + '/LOG_GPU.log'\n",
    "sys.stdout = open(log_file, 'w')\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "data_folder = \"/home/qklmn/data/\"\n",
    "# data_folder = \"/data1/simulations/\"\n",
    "\n",
    "print(\"Loading training data...\")\n",
    "# traj_300 = io.read(\"/data1/simulations/datasets/rotors/different_temperatures/300/OUTCAR\", format=\"vasp-out\", index = \":\")\n",
    "# traj_600 = io.read(\"/data1/simulations/datasets/rotors/different_temperatures/600/OUTCAR\", format=\"vasp-out\", index = \":\")\n",
    "# traj_900 = io.read(\"/data1/simulations/datasets/rotors/different_temperatures/900/OUTCAR\", format=\"vasp-out\", index = \":\")\n",
    "# traj_1200 = io.read(\"/data1/simulations/datasets/rotors/different_temperatures/1200/OUTCAR\", format=\"vasp-out\", index = \":\")\n",
    "# traj_1500 = io.read(\"/data1/simulations/datasets/rotors/different_temperatures/1200/OUTCAR\", format=\"vasp-out\", index = \":\")\n",
    "# traj_1800 = io.read(\"/data1/simulations/datasets/rotors/different_temperatures/1800/OUTCAR\", format=\"vasp-out\", index = \":\")\n",
    "# traj_2100 = io.read(\"/data1/simulations/datasets/rotors/different_temperatures/2100/OUTCAR\", format=\"vasp-out\", index = \":\")\n",
    "# print(len(traj_300), len(traj_600), len(traj_900), len(traj_1200), len(traj_1500), len(traj_1800), len(traj_2100))\n",
    "\n",
    "\n",
    "traj_300 = io.read(data_folder + \"datasets/rotors/different_temperatures/300/OUTCAR\", format=\"vasp-out\", index = \":\")\n",
    "# traj_600 = io.read(data_folder + \"datasets/rotors/different_temperatures/600/OUTCAR\", format=\"vasp-out\", index = \":\")\n",
    "# traj_900 = io.read(data_folder + \"datasets/rotors/different_temperatures/900/OUTCAR\", format=\"vasp-out\", index = \":\")\n",
    "# traj_1200 = io.read(data_folder + \"datasets/rotors/different_temperatures/1200/OUTCAR\", format=\"vasp-out\", index = \":\")\n",
    "traj_1500 = io.read(data_folder + \"datasets/rotors/different_temperatures/1500/OUTCAR\", format=\"vasp-out\", index = \":\")\n",
    "traj_1800 = io.read(data_folder + \"datasets/rotors/different_temperatures/1800/OUTCAR\", format=\"vasp-out\", index = \":\")\n",
    "traj_2100 = io.read(data_folder + \"datasets/rotors/different_temperatures/2100/OUTCAR\", format=\"vasp-out\", index = \":\")\n",
    "\n",
    "traj_train = traj_1800.copy() + traj_2100.copy()\n",
    "# traj_train = traj_2100.copy()\n",
    "# training_indices = np.sort(  np.arange(0, 500, 5) )  \n",
    "# traj_train = [traj_md[i] for i in training_indices]\n",
    "\n",
    "traj_test = traj_300[400:420].copy()\n",
    "# test_indices = np.sort(  np.random.choice(np.arange(0,92795), 200, replace=False) ) \n",
    "# test_indices = np.sort(  np.arange(400,410,1) ) \n",
    "# traj_test = [traj_md[i] for i in test_indices]\n",
    "\n",
    "\n",
    "import os\n",
    "machine_name = os.uname()[1]\n",
    "\n",
    "import wandb\n",
    "\n",
    "\n",
    "wandb.init(project=\"rotor-gp\", save_code=True, notes=\"hello\", id=machine_name, mode='disabled')\n",
    "\n",
    "\n",
    "# moving_atoms = [\n",
    "#     #   upper layer rings\n",
    "#     43,47,49,53,57,59, 45,51,55,61, \n",
    "#     77,73,69,67,83,79, 75,71,85,81,\n",
    "#     33,29,25,23,39,35, 31,27,41,37,\n",
    "#     #   lower layer rings\n",
    "#     76,78,82,66,68,72, 80,84,70,74,\n",
    "#     32,34,38,22,24,28, 36,40,26,30,\n",
    "#     46,42,58,56,52,48, 44,60,54,50\n",
    "#       ]\n",
    "# rings_carbons = [\n",
    "#     #   upper layer rings\n",
    "#     43,47,49,53,57,59,  \n",
    "#     77,73,69,67,83,79, \n",
    "#     33,29,25,23,39,35, \n",
    "#     #   lower layer rings\n",
    "#     76,78,82,66,68,72, \n",
    "#     32,34,38,22,24,28, \n",
    "#     46,42,58,56,52,48,       \n",
    "#     ]\n",
    "rings_hydrogens = [\n",
    "    #   upper layer rings\n",
    "    123, 125, 127, 129,\n",
    "    133, 135, 137, 139, \n",
    "    115, 117, 119, 121,  \n",
    "    #   lower layer rings\n",
    "    114, 116, 118, 120, \n",
    "    122, 124, 126, 128,  \n",
    "    132, 134, 136, 138, \n",
    "    ]\n",
    "\n",
    "rings_carbons = [\n",
    "#   upper layer rings\n",
    "25, 27, 29, 31, 33, 35,\n",
    "39, 41, 43, 45, 47, 49,\n",
    "13, 15, 17, 19, 21, 23,\n",
    "#   lower layer rings\n",
    "12, 14, 16, 18, 20, 22,\n",
    "24, 26, 28, 30, 32, 34,\n",
    "38, 40, 42, 44, 46, 48,\n",
    "]\n",
    "\n",
    "from fande.data import FandeDataModuleASE\n",
    "\n",
    "atoms = traj_train[0].copy()\n",
    "H_atoms = [atom.index for atom in atoms if atom.symbol == \"H\"]\n",
    "C_atoms = [atom.index for atom in atoms if atom.symbol == \"C\"]\n",
    "O_atoms = [atom.index for atom in atoms if atom.symbol == \"O\"]\n",
    "N_atoms = [atom.index for atom in atoms if atom.symbol == \"N\"]\n",
    "Si_atoms = [atom.index for atom in atoms if atom.symbol == \"Si\"]\n",
    "\n",
    "H_atoms = rings_hydrogens\n",
    "C_atoms = rings_carbons\n",
    "\n",
    "atomic_groups = [H_atoms, C_atoms,] #O_atoms, N_atoms, Si_atoms]\n",
    "\n",
    "\n",
    "## Train data:\n",
    "energies_train = np.zeros(len(traj_train) )\n",
    "forces_train = np.zeros( (len(traj_train), len(traj_train[0]), 3 ) )\n",
    "for i, snap in enumerate(traj_train):\n",
    "    energies_train[i] = snap.get_potential_energy()\n",
    "    forces_train[i] = snap.get_forces()\n",
    "train_data = {'trajectory': traj_train, 'energies': energies_train, 'forces': forces_train}\n",
    "## Test data:\n",
    "energies_test = np.zeros(len(traj_test) )\n",
    "forces_test = np.zeros( (len(traj_test), len(traj_test[0]), 3 ) )\n",
    "for i, snap in enumerate(traj_test):\n",
    "    energies_test[i] = snap.get_potential_energy()\n",
    "    forces_test[i] = snap.get_forces()\n",
    "test_data = {'trajectory': traj_test, 'energies': energies_test, 'forces': forces_test}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_centers_positions = H_atoms + C_atoms#list(range(len(atoms)))\n",
    "train_derivatives_positions = H_atoms + C_atoms#list(range(len(atoms)))\n",
    "\n",
    "# Hyperparameters:\n",
    "hparams = {}\n",
    "\n",
    "# Descriptors parameters:\n",
    "soap_params = {\n",
    "    'species': [\"H\", \"C\", \"O\", \"N\", \"Si\"],\n",
    "    'periodic': True,\n",
    "    'rcut': 3.0,\n",
    "    'sigma': 0.5,\n",
    "    'nmax': 4,\n",
    "    'lmax': 4,\n",
    "    'average': \"off\",\n",
    "    'crossover': True,\n",
    "    'dtype': \"float64\",\n",
    "    'n_jobs': 10,\n",
    "    'sparse': False,\n",
    "    'positions': [7, 11, 15] # ignored\n",
    "}\n",
    "\n",
    "fdm = FandeDataModuleASE(train_data, test_data, hparams)\n",
    "\n",
    "fdm.calculate_invariants_librascal(\n",
    "    soap_params,\n",
    "    atomic_groups = atomic_groups,\n",
    "    centers_positions = train_centers_positions, \n",
    "    derivatives_positions = train_derivatives_positions,\n",
    "    same_centers_derivatives=True,\n",
    "    frames_per_batch=1,\n",
    "    calculation_context=\"train\")\n",
    "\n",
    "fdm.calculate_invariants_librascal(\n",
    "    soap_params,\n",
    "    atomic_groups = atomic_groups,\n",
    "    centers_positions = train_centers_positions, \n",
    "    derivatives_positions = train_derivatives_positions,\n",
    "    same_centers_derivatives=True,\n",
    "    frames_per_batch=1,\n",
    "    calculation_context=\"test\")\n",
    "\n",
    "\n",
    "\n",
    "for g in range(len(atomic_groups)):\n",
    "    print(\"\\n-----------\")\n",
    "    print(\"Group \", g)\n",
    "    print(\"-----------\")\n",
    "    plt.plot(fdm.train_F[g].cpu()[1::1], linestyle = 'None', marker='o', label='train')\n",
    "    plt.plot(fdm.test_F[g].cpu()[1::1], linestyle = 'None', marker='x', label='test')\n",
    "    plt.savefig(\"tran_test_forces_group_\" + str(g) + \".png\")\n",
    "    plt.close()\n",
    "\n",
    "    plt.hist(fdm.test_F[g].cpu().numpy())\n",
    "    plt.savefig(\"histogram_forces_group_\" + str(g) + \".png\")\n",
    "    plt.close()\n",
    "    print(f\"Number of training points for group {g}: \", fdm.train_DX[g].shape[-2])\n",
    "    print(f\"Number of test points for group {g}: \", fdm.test_DX[g].shape[-2])\n",
    "\n",
    "\n",
    "\n",
    "# import torch\n",
    "# import numpy as np\n",
    "\n",
    "# total_training_random_samples = 10\n",
    "# high_force_samples = 5\n",
    "# random_samples = total_training_random_samples - high_force_samples\n",
    "\n",
    "# indices_high_force = torch.concat( \n",
    "#     (torch.topk(fdm.test_F[0], high_force_samples//2, largest=True)[1],  \n",
    "#      torch.topk(fdm.test_F[0], high_force_samples//2, largest=False)[1]) ).cpu().numpy()\n",
    "\n",
    "# ind_slice = np.sort(  \n",
    "#     np.random.choice(np.setdiff1d(np.arange(0, fdm.train_F[0].shape[0]), indices_high_force), random_samples, replace=False) )\n",
    "\n",
    "# indices = np.concatenate((ind_slice, indices_high_force))\n",
    "# indices = np.unique(indices)\n",
    "# print(\"High force indices: \", indices_high_force)\n",
    "# print(ind_slice)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "# seed_everything(42, workers=True)\n",
    "import torch\n",
    "\n",
    "hparams = {\n",
    "    'dtype' : 'float32',\n",
    "    'device' : 'gpu'\n",
    "}\n",
    "\n",
    "\n",
    "per_model_hparams = []\n",
    "\n",
    "train_DX = fdm.train_DX\n",
    "train_F = fdm.train_F\n",
    "test_DX = fdm.test_DX\n",
    "test_F = fdm.test_F\n",
    "\n",
    "\n",
    "\n",
    "### Prepare data loaders and specify how to sample data for each group:\n",
    "total_samples_per_group = [\n",
    "    1_000, #H\n",
    "    10_000, #C\n",
    "    # 5_000, #N\n",
    "    # 5_000, #O\n",
    "    # 5_000, #Si    \n",
    "    ]\n",
    "\n",
    "high_force_samples_per_group = [\n",
    "    0,\n",
    "    0,\n",
    "    # 0,\n",
    "    # 0,\n",
    "    # 0,\n",
    "    ]\n",
    "\n",
    "train_data_loaders = fdm.prepare_train_data_loaders(\n",
    "    total_samples_per_group=total_samples_per_group,\n",
    "    high_force_samples_per_group=high_force_samples_per_group)\n",
    "\n",
    "hparams['train_indices'] = fdm.train_indices\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qklmn/mambaforge/envs/gpuenv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning:\n",
      "\n",
      "The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 56 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\n",
      "/home/qklmn/mambaforge/envs/gpuenv/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:280: PossibleUserWarning:\n",
      "\n",
      "The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_H_hparams = {\n",
    "    'atomic_group' : H_atoms,\n",
    "    'dtype' : hparams['dtype'],\n",
    "    'device' : hparams['device'],\n",
    "    'num_epochs' : 300,\n",
    "    'learning_rate' : 0.01,\n",
    "    'soap_dim' : fdm.train_DX[0].shape[-1],\n",
    "    'soap_params' : soap_params,\n",
    "}\n",
    "\n",
    "model_C_hparams = {\n",
    "    'atomic_group' : C_atoms,\n",
    "    'dtype' : hparams['dtype'],\n",
    "    'device' : hparams['device'],\n",
    "    'num_epochs' : 200,\n",
    "    'learning_rate' : 0.01,\n",
    "    'soap_dim' : fdm.train_DX[1].shape[-1],\n",
    "    'soap_params' : soap_params,\n",
    "}\n",
    "\n",
    "# model_N_hparams = {\n",
    "#     'atomic_group' : N_atoms,\n",
    "#     'dtype' : hparams['dtype'],\n",
    "#     'device' : hparams['device'],\n",
    "#     'num_epochs' : 1,\n",
    "#     'learning_rate' : 0.01,\n",
    "#     'soap_dim' : fdm.train_DX[1].shape[-1],\n",
    "#     'soap_params' : soap_params,\n",
    "# }\n",
    "\n",
    "# model_O_hparams = {\n",
    "#     'atomic_group' : O_atoms,\n",
    "#     'dtype' : hparams['dtype'],\n",
    "#     'device' : hparams['device'],\n",
    "#     'num_epochs' : 1,\n",
    "#     'learning_rate' : 0.01,\n",
    "#     'soap_dim' : fdm.train_DX[1].shape[-1],\n",
    "#     'soap_params' : soap_params,\n",
    "# }\n",
    "\n",
    "# model_Si_hparams = {\n",
    "#     'atomic_group' : Si_atoms,\n",
    "#     'dtype' : hparams['dtype'],\n",
    "#     'device' : hparams['device'],\n",
    "#     'num_epochs' : 1, \n",
    "#     'learning_rate' : 0.01,\n",
    "#     'soap_dim' : fdm.train_DX[1].shape[-1],\n",
    "#     'soap_params' : soap_params,\n",
    "# }\n",
    "\n",
    "\n",
    "hparams['per_model_hparams'] = [ model_H_hparams, model_C_hparams,] # model_N_hparams, model_O_hparams, model_Si_hparams ] # access per_model_hparams by model.model_id\n",
    "hparams['soap_dim'] = fdm.train_DX[0].shape[-1]\n",
    "\n",
    "\n",
    "#####################################################################\n",
    "\n",
    "from fande.models import ModelForces, GroupModelForces, ModelEnergies, MyCallbacks\n",
    "\n",
    "model_H = ModelForces(\n",
    "    train_x = train_data_loaders[0].dataset[:][0],\n",
    "    train_y = train_data_loaders[0].dataset[:][1],\n",
    "    atomic_group = H_atoms,\n",
    "    hparams = hparams,\n",
    "    id=0)\n",
    "\n",
    "model_C = ModelForces(\n",
    "    train_x = train_data_loaders[1].dataset[:][0],\n",
    "    train_y = train_data_loaders[1].dataset[:][1],\n",
    "    atomic_group = C_atoms,\n",
    "    hparams = hparams,\n",
    "    id=1)\n",
    "\n",
    "# model_N = ModelForces(\n",
    "#     train_x = train_data_loaders[2].dataset[:][0],\n",
    "#     train_y = train_data_loaders[2].dataset[:][1],\n",
    "#     atomic_group = N_atoms,\n",
    "#     hparams = hparams,\n",
    "#     id=2)\n",
    "\n",
    "# model_O = ModelForces(\n",
    "#     train_x = train_data_loaders[3].dataset[:][0],\n",
    "#     train_y = train_data_loaders[3].dataset[:][1],\n",
    "#     atomic_group = O_atoms,\n",
    "#     hparams = hparams,\n",
    "#     id=3)\n",
    "\n",
    "# model_Si = ModelForces(\n",
    "#     train_x = train_data_loaders[4].dataset[:][0],\n",
    "#     train_y = train_data_loaders[4].dataset[:][1],\n",
    "#     atomic_group = Si_atoms,\n",
    "#     hparams = hparams,\n",
    "#     id=4)\n",
    "\n",
    "\n",
    "AG_force_model = GroupModelForces(\n",
    "    models=[model_H, model_C], # model_N, model_O, model_Si],\n",
    "    train_data_loaders = train_data_loaders,\n",
    "    hparams=hparams)\n",
    "\n",
    "AG_force_model.fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.count_nonzero(fdm.train_DX[2][100])\n",
    "\n",
    "# fdm.train_DX[1].shape\n",
    "\n",
    "len(AG_force_model.train_data_loaders[1].dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[114]"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qklmn/mambaforge/envs/gpuenv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning:\n",
      "\n",
      "The dataloader, predict_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 56 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### TESTING PREDICITONS ###\n",
    "print('Testing performance with (meta-)dynamics run...')\n",
    "\n",
    "from fande.predict import PredictorASE\n",
    "\n",
    "model_e = None\n",
    "trainer_e = None\n",
    "\n",
    "AG_force_model.eval()\n",
    "\n",
    "predictor = PredictorASE(\n",
    "            fdm,\n",
    "            model_e,\n",
    "            trainer_e,\n",
    "            AG_force_model,\n",
    "            # trainer_f,\n",
    "            hparams,\n",
    "            soap_params\n",
    ")\n",
    "\n",
    "predictor.test_errors(view_worst_atoms=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qklmn/mambaforge/envs/gpuenv/lib/python3.10/site-packages/ase/md/verlet.py:42: FutureWarning:\n",
      "\n",
      "dt variable is deprecated; please use timestep.\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.94it/s]\n",
      "/home/qklmn/mambaforge/envs/gpuenv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning:\n",
      "\n",
      "The dataloader, predict_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 56 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.29it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.33it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.28it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.22it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.04it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.32it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.27it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.28it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.25it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.21it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.27it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.33it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.30it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.34it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.29it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.31it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.33it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.85it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.13it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.28it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.33it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.28it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.34it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.28it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.25it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.29it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.27it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.27it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.30it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.26it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.30it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.27it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.30it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.28it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.27it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.28it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.26it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.30it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.23it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.29it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.30it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.31it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.29it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.34it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.30it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.29it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.26it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.32it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.05it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.27it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.25it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.26it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.26it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.30it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.26it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.28it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.26it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.29it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.29it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.29it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.31it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.29it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.29it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.21it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.26it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.26it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.30it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.25it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.32it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.27it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.31it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.24it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.33it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.21it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.33it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.19it/s]\n",
      "/home/qklmn/mambaforge/envs/gpuenv/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:52: UserWarning:\n",
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[288], line 86\u001b[0m\n\u001b[1;32m     47\u001b[0m dyn \u001b[39m=\u001b[39m VelocityVerlet(\n\u001b[1;32m     48\u001b[0m     atoms,\n\u001b[1;32m     49\u001b[0m     dt \u001b[39m=\u001b[39m \u001b[39m0.5\u001b[39m\u001b[39m*\u001b[39munits\u001b[39m.\u001b[39mfs,\n\u001b[1;32m     50\u001b[0m     trajectory\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmd_run/md_test.traj\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     51\u001b[0m     logfile\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmd_run/md_log.log\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     52\u001b[0m )\n\u001b[1;32m     54\u001b[0m \u001b[39m# dyn = NPT(\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[39m#     atoms,\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m#     # dt = 0.5*units.fs,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[39m#                fixcm=True, trajectory='md_run/md_test.traj',\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[39m#                logfile=\"md_run/md_log.log\")\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m dyn\u001b[39m.\u001b[39;49mrun(\u001b[39m500\u001b[39;49m)\n\u001b[1;32m     88\u001b[0m \u001b[39m# # Structure optimization:\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[39m# dyn = BFGS(\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[39m#     atoms,\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[39m#     trajectory=\"../results/test/md_runs/md_test.traj\",\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[39m#     logfile=\"../results/test/md_runs/md_log.log\",)\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[39m# dyn.run(fmax=0.1)\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m ALL JOBS WITHIN PYTHON SCRIPT ARE DONE! \u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/mambaforge/envs/gpuenv/lib/python3.10/site-packages/ase/md/md.py:137\u001b[0m, in \u001b[0;36mMolecularDynamics.run\u001b[0;34m(self, steps)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\" Call Dynamics.run and adjust max_steps \"\"\"\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_steps \u001b[39m=\u001b[39m steps \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnsteps\n\u001b[0;32m--> 137\u001b[0m \u001b[39mreturn\u001b[39;00m Dynamics\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[0;32m~/mambaforge/envs/gpuenv/lib/python3.10/site-packages/ase/optimize/optimize.py:156\u001b[0m, in \u001b[0;36mDynamics.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    150\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Run dynamics algorithm.\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \n\u001b[1;32m    152\u001b[0m \u001b[39m    This method will return when the forces on all individual\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[39m    atoms are less than *fmax* or when the number of steps exceeds\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[39m    *steps*.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 156\u001b[0m     \u001b[39mfor\u001b[39;00m converged \u001b[39min\u001b[39;00m Dynamics\u001b[39m.\u001b[39mirun(\u001b[39mself\u001b[39m):\n\u001b[1;32m    157\u001b[0m         \u001b[39mpass\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     \u001b[39mreturn\u001b[39;00m converged\n",
      "File \u001b[0;32m~/mambaforge/envs/gpuenv/lib/python3.10/site-packages/ase/optimize/optimize.py:135\u001b[0m, in \u001b[0;36mDynamics.irun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[39m# run the algorithm until converged or max_steps reached\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconverged() \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnsteps \u001b[39m<\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_steps:\n\u001b[1;32m    133\u001b[0m \n\u001b[1;32m    134\u001b[0m     \u001b[39m# compute the next step\u001b[39;00m\n\u001b[0;32m--> 135\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m    136\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnsteps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    138\u001b[0m     \u001b[39m# let the user inspect the step and change things before logging\u001b[39;00m\n\u001b[1;32m    139\u001b[0m     \u001b[39m# and predicting the next step\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/gpuenv/lib/python3.10/site-packages/ase/md/verlet.py:75\u001b[0m, in \u001b[0;36mVelocityVerlet.step\u001b[0;34m(self, forces)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[39m# We need to store the momenta on the atoms before calculating\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[39m# the forces, as in a parallel Asap calculation atoms may\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39m# migrate during force calculations, and the momenta need to\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[39m# migrate along with the atoms.\u001b[39;00m\n\u001b[1;32m     73\u001b[0m atoms\u001b[39m.\u001b[39mset_momenta(p, apply_constraint\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m---> 75\u001b[0m forces \u001b[39m=\u001b[39m atoms\u001b[39m.\u001b[39;49mget_forces(md\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     77\u001b[0m \u001b[39m# Second part of RATTLE will be done here:\u001b[39;00m\n\u001b[1;32m     78\u001b[0m atoms\u001b[39m.\u001b[39mset_momenta(atoms\u001b[39m.\u001b[39mget_momenta() \u001b[39m+\u001b[39m \u001b[39m0.5\u001b[39m \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdt \u001b[39m*\u001b[39m forces)\n",
      "File \u001b[0;32m~/mambaforge/envs/gpuenv/lib/python3.10/site-packages/ase/atoms.py:788\u001b[0m, in \u001b[0;36mAtoms.get_forces\u001b[0;34m(self, apply_constraint, md)\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_calc \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    787\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mAtoms object has no calculator.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 788\u001b[0m forces \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_calc\u001b[39m.\u001b[39;49mget_forces(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    790\u001b[0m \u001b[39mif\u001b[39;00m apply_constraint:\n\u001b[1;32m    791\u001b[0m     \u001b[39m# We need a special md flag here because for MD we want\u001b[39;00m\n\u001b[1;32m    792\u001b[0m     \u001b[39m# to skip real constraints but include special \"constraints\"\u001b[39;00m\n\u001b[1;32m    793\u001b[0m     \u001b[39m# Like Hookean.\u001b[39;00m\n\u001b[1;32m    794\u001b[0m     \u001b[39mfor\u001b[39;00m constraint \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconstraints:\n",
      "File \u001b[0;32m/misc/home/qklmn/repos/rotor-gp/code/../../fande/fande/ase/fande_calc.py:188\u001b[0m, in \u001b[0;36mFandeCalc.get_forces\u001b[0;34m(self, atoms)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_forces\u001b[39m(\u001b[39mself\u001b[39m, atoms):\n\u001b[0;32m--> 188\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mupdate(atoms)\n\u001b[1;32m    189\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforces\u001b[39m.\u001b[39mcopy()\n",
      "File \u001b[0;32m/misc/home/qklmn/repos/rotor-gp/code/../../fande/fande/ase/fande_calc.py:185\u001b[0m, in \u001b[0;36mFandeCalc.update\u001b[0;34m(self, atoms)\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcalculate(atoms)\n\u001b[1;32m    182\u001b[0m \u001b[39melif\u001b[39;00m ((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpositions \u001b[39m!=\u001b[39m atoms\u001b[39m.\u001b[39mget_positions())\u001b[39m.\u001b[39many() \u001b[39mor\u001b[39;00m\n\u001b[1;32m    183\u001b[0m       (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpbc \u001b[39m!=\u001b[39m atoms\u001b[39m.\u001b[39mget_pbc())\u001b[39m.\u001b[39many() \u001b[39mor\u001b[39;00m\n\u001b[1;32m    184\u001b[0m       (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcell \u001b[39m!=\u001b[39m atoms\u001b[39m.\u001b[39mget_cell())\u001b[39m.\u001b[39many()):\n\u001b[0;32m--> 185\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcalculate(atoms)\n",
      "File \u001b[0;32m/misc/home/qklmn/repos/rotor-gp/code/../../fande/fande/ase/fande_calc.py:129\u001b[0m, in \u001b[0;36mFandeCalc.calculate\u001b[0;34m(self, atoms, properties, system_changes)\u001b[0m\n\u001b[1;32m    124\u001b[0m stresses \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((natoms, \u001b[39m3\u001b[39m, \u001b[39m3\u001b[39m))\n\u001b[1;32m    126\u001b[0m \u001b[39m# forces, forces_var = self.predictor.predict_forces_single(self.atoms)\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[39m# print(\"Calculating FORCES!\")\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m forces \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredictor\u001b[39m.\u001b[39;49mpredict_forces_single_snapshot_r(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49matoms\u001b[39m.\u001b[39;49mcopy())\n\u001b[1;32m    130\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforces \u001b[39m=\u001b[39m forces\n\u001b[1;32m    132\u001b[0m \u001b[39m# comparing with supporting calculation\u001b[39;00m\n",
      "File \u001b[0;32m/misc/home/qklmn/repos/rotor-gp/code/../../fande/fande/predict/predictors_ase.py:578\u001b[0m, in \u001b[0;36mPredictorASE.predict_forces_single_snapshot_r\u001b[0;34m(self, snapshot, atomic_groups)\u001b[0m\n\u001b[1;32m    575\u001b[0m trainer_f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mag_force_model\u001b[39m.\u001b[39mtrainers[idx]\n\u001b[1;32m    577\u001b[0m \u001b[39m# model.eval()\u001b[39;00m\n\u001b[0;32m--> 578\u001b[0m res \u001b[39m=\u001b[39m trainer_f\u001b[39m.\u001b[39;49mpredict(model, test_dl)[\u001b[39m0\u001b[39;49m]\n\u001b[1;32m    580\u001b[0m predictions_torch \u001b[39m=\u001b[39m res\u001b[39m.\u001b[39mmean\n\u001b[1;32m    582\u001b[0m predictions \u001b[39m=\u001b[39m res\u001b[39m.\u001b[39mmean\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "### MD with fande calc\n",
    "from fande.ase import FandeCalc\n",
    "from ase.units import Bohr,Rydberg,kJ,kB,fs,Hartree,mol,kcal\n",
    "\n",
    "\n",
    "# from ase.geometry.analysis import Analysis\n",
    "from ase.constraints import FixAtoms, FixBondLengths\n",
    "from ase.optimize import BFGS\n",
    "from ase import units\n",
    "from ase.io import read\n",
    "import logging\n",
    "from ase.md.velocitydistribution import MaxwellBoltzmannDistribution\n",
    "from ase.md.verlet import VelocityVerlet\n",
    "from ase.md.langevin import Langevin\n",
    "from ase.md.npt import NPT\n",
    "from ase.md.nptberendsen import NPTBerendsen\n",
    "from ase.md.nvtberendsen import NVTBerendsen\n",
    "\n",
    "\n",
    "logging.getLogger(\"pytorch_lightning\").setLevel(logging.ERROR) # logging.ERROR to disable or INFO\n",
    "\n",
    "# traj_md = read('../results/test/machine_learning/dftb_opt_1000_six_rings.traj', index=\":\")\n",
    "# traj_opt = read('../results/test/machine_learning/opt.traj', index=\":\")\n",
    "\n",
    "# atoms = fdm.mol_traj[10].copy()\n",
    "# atoms = traj_md[300].copy()\n",
    "# atoms = traj_opt[-1].copy()\n",
    "atoms = traj_test[10].copy()\n",
    "atoms.set_pbc(True)\n",
    "\n",
    "\n",
    "moving_atoms = model_H.atomic_group + model_C.atomic_group\n",
    "fixed_atoms = list( set(range(264)) - set(moving_atoms) )\n",
    "fix_atoms = FixAtoms(indices=fixed_atoms)\n",
    "atoms.set_constraint(fix_atoms)\n",
    "\n",
    "\n",
    "atoms.calc = FandeCalc(predictor)\n",
    "# atoms.calc.set_atomic_groups([rings_carbons, rings_hydrogens], titles=[\"Rings carbons\", \"Rings hydrogens\"])\n",
    "# atoms.calc.set_forces_errors_plot_file(\"../results/test/md_runs/forces_errors.png\", loginterval=1)\n",
    "# atoms.calc = LennardJones()\n",
    "\n",
    "os.makedirs(\"md_run/\", exist_ok=True)\n",
    "\n",
    "# Verlet dynamics:\n",
    "MaxwellBoltzmannDistribution(atoms, temperature_K=1000)\n",
    "dyn = VelocityVerlet(\n",
    "    atoms,\n",
    "    dt = 0.5*units.fs,\n",
    "    trajectory=\"md_run/md_test.traj\",\n",
    "    logfile=\"md_run/md_log.log\",\n",
    ")\n",
    "\n",
    "# dyn = NPT(\n",
    "#     atoms,\n",
    "#     # dt = 0.5*units.fs,\n",
    "#     timestep=0.1,\n",
    "#     temperature_K=300,\n",
    "#     externalstress=0.0,\n",
    "#     trajectory=\"../results/test/md_runs/md_test.traj\",\n",
    "#     logfile=\"../results/test/md_runs/md_log.log\",\n",
    "# )\n",
    "\n",
    "# dyn = NPTBerendsen(atoms, timestep=0.1 * units.fs, temperature_K=300,\n",
    "#                    taut=100 * units.fs, pressure_au=1.01325 * units.bar,\n",
    "#                    taup=1000 * units.fs, compressibility=4.57e-5 / units.bar,\n",
    "#                    trajectory=\"../results/test/md_runs/md_test.traj\",\n",
    "#                    logfile=\"../results/test/md_runs/md_log.log\",)\n",
    "\n",
    "# import os\n",
    "\n",
    "\n",
    "# dyn = NVTBerendsen(atoms, 0.5 * units.fs, 300, taut=0.5*1000*units.fs, \n",
    "#                    trajectory=\"md_run/md_test.traj\",   \n",
    "#                    logfile=\"md_run/md_log.log\")\n",
    "\n",
    "# dyn.run(100)\n",
    "\n",
    "# Langevin dynamics:\n",
    "# https://databases.fysik.dtu.dk/ase/tutorials/md/md.html\n",
    "# MaxwellBoltzmannDistribution(atoms, temperature_K=300)\n",
    "# dyn = Langevin(atoms, 0.1*fs, temperature_K=0.1/units.kB, friction=0.1,\n",
    "#                fixcm=True, trajectory='md_run/md_test.traj',\n",
    "#                logfile=\"md_run/md_log.log\")\n",
    "\n",
    "dyn.run(10_000)\n",
    "\n",
    "# # Structure optimization:\n",
    "# dyn = BFGS(\n",
    "#     atoms,\n",
    "#     trajectory=\"../results/test/md_runs/md_test.traj\",\n",
    "#     logfile=\"../results/test/md_runs/md_log.log\",)\n",
    "# dyn.run(fmax=0.1)\n",
    "\n",
    "\n",
    "print(\" ALL JOBS WITHIN PYTHON SCRIPT ARE DONE! \")\n",
    "\n",
    "print(\"TIMING: \", time.time()-start_time, \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ase.units import Bohr,Rydberg,kJ,kB,fs,Hartree,mol,kcal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0180505671156725"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Error while calling W&B API: An internal error occurred. Please contact support. (<Response [500]>)\n"
     ]
    }
   ],
   "source": [
    "0.1/fs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
